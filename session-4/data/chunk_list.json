[
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Fine-Tuning BERT for Text Classification",
        "text": "Although today’s 100B+ parameter transformer models are state-of-the-art in AI, there’s still much we can accomplish with smaller (< 1B parameter) models. In this article, I will walk through one such example, fine-tuning BERT (110M parameters) to classify phishing URLs. I’ll start by covering key concepts and then share example Python code."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Fine-tuning",
        "text": "Fine-tuning involves adapting a pre-trained model to a particular use case through additional training."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Fine-tuning",
        "text": "Pre-trained models are developed via unsupervised learning, which precludes the need for large-scale labeled datasets. Fine-tuned models can then exploit pre-trained model representations to significantly reduce training costs and improve model performance compared to training from scratch [1]."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Fine-tuning",
        "text": "Splitting the training process into multiple phases has led to today’s state-of-the-art transformer models, such as GPT-4o, Claude, and Llama 3.2. It also enables the democratization of AI since the expensive undertaking of model pre-training can be done by specialized research labs, who can then make these models publicly available for fine-tuning."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "BERT",
        "text": "While model fine-tuning gained tremendous popularity post-ChatGPT, it’s been around since (at least) 2015 [2]. One of the early language models developed specifically for fine-tuning was Google’s BERT model, which was pre-trained on two unsupervised tasks: 1) masked language modeling (MLM) and 2) next sentence prediction [1]."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "BERT",
        "text": "The MLM pre-training task consists of predicting arbitrarily masked words in a sequence. This is in contrast to causal language modeling, which is restricted to predicting the word at the end of a sequence. Therefore, MLM enables models to leverage more context (i.e. text before AND after the masked word) to make predictions [1]."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "BERT",
        "text": "Next sentence prediction is important for downstream tasks that require understanding the relationship between two sentences (e.g., Question Answering and Semantic Similarity). This is implemented using special input tokens to distinguish the sentence prediction task from the MLM [1]."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "BERT",
        "text": "These pre-training tasks enable BERT to be fine-tuned on a wide range of tasks such as sentiment analysis, sentence similarity, question answering, named entity recognition, common sense reasoning, and many others [1]."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Text Classification",
        "text": "Many of the tasks mentioned above (e.g. sentiment analysis, sentence similarity, named entity recognition) fall under the category of text classification, i.e., assigning a label to input text sequences."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Text Classification",
        "text": "There are countless practical applications of text classification, such as detecting spam in emails, categorizing IT support tickets, detecting toxic or harmful speech, and analyzing the sentiment of customer reviews. While each of these tasks is practically very different, their implementations are almost identical from a technical standpoint."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Example Code: Fine-tuning BERT for Phishing URL Identification",
        "text": "Here, we will walk through an example of BERT fine-tuning to classify phishing URLs. We will use the bert-base-uncased model freely available on the Hugging Face (HF) hub."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Example Code: Fine-tuning BERT for Phishing URL Identification",
        "text": "The model consists of 110M parameters, of which we will only train a small percentage. Therefore, this example should easily run on most consumer hardware (no GPU required)."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Example Code: Fine-tuning BERT for Phishing URL Identification",
        "text": "The fine-tuned model is also available on the HF hub, and an example notebook is available on GitHub."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Example Code: Fine-tuning BERT for Phishing URL Identification",
        "text": "We’ll start by importing a few handy libraries."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Example Code: Fine-tuning BERT for Phishing URL Identification",
        "text": "Next, we’ll load the training dataset. It consists of 3,000 text-label pairs with a 70–15–15 train-test-validation split. The data are originally from here (open database license)."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Example Code: Fine-tuning BERT for Phishing URL Identification",
        "text": "The Transformer library makes it super easy to load and adapt pre-trained models. Here’s what that looks like for the BERT model."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Example Code: Fine-tuning BERT for Phishing URL Identification",
        "text": "When we load a model like this, all the parameters will be set as trainable by default. However, training all 110M parameters will be computationally costly and potentially unnecessary."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Example Code: Fine-tuning BERT for Phishing URL Identification",
        "text": "Instead, we can freeze most of the model parameters and only train the model’s final layer and classification head."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Example Code: Fine-tuning BERT for Phishing URL Identification",
        "text": "Next, we will need to preprocess our data. This will consist of two key operations: tokenizing the URLs (i.e., converting them into integers) and truncating them."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Example Code: Fine-tuning BERT for Phishing URL Identification",
        "text": "Another important step is creating a data collator that will dynamically pad token sequences in a batch during training so they have the same length. We can do this in one line of code."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Example Code: Fine-tuning BERT for Phishing URL Identification",
        "text": "As a final step before training, we can define a function to compute a set of metrics to help us monitor training progress. Here, we will consider model accuracy and AUC."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Example Code: Fine-tuning BERT for Phishing URL Identification",
        "text": "Now, we are ready to fine-tune our model. We start by defining hyperparameters and other training arguments."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Example Code: Fine-tuning BERT for Phishing URL Identification",
        "text": "Then, we pass our training arguments into a trainer class and train the model."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Example Code: Fine-tuning BERT for Phishing URL Identification",
        "text": "The training results are shown below. We can see that the training and validation loss are monotonically decreasing while the accuracy and AUC increase with each epoch."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Example Code: Fine-tuning BERT for Phishing URL Identification",
        "text": "As a final test, we can evaluate the performance of the model on the independent validation data, i.e., data not used for training or setting hyperparameters."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Example Code: Fine-tuning BERT for Phishing URL Identification",
        "text": "Bonus: Although a 110M parameter model is tiny compared to modern language models, we can reduce its computational requirements using model compression techniques. I cover how to reduce the memory footprint model by 7X in the article below."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Conclusion",
        "text": "Fine-tuning pre-trained models is a powerful paradigm for developing better models at a lower cost than training them from scratch. Here, we saw how to do this with BERT using the Hugging Face Transformers library."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Conclusion",
        "text": "While the example code was for URL classification, it can be readily adapted to other text classification tasks."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Conclusion",
        "text": "My website: https://www.shawhintalebi.com/"
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Conclusion",
        "text": "[1] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Conclusion",
        "text": "[2] Semi-supervised Sequence Learning"
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Conclusion",
        "text": "By Shaw Talebi on October 17, 2024."
    },
    {
        "article_title": "Fine-Tuning BERT for Text Classification",
        "section": "Conclusion",
        "text": "Exported from Medium on December 2, 2024."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "A Practical Introduction to LLMs",
        "text": "This is the first article in a series on using Large Language Models (LLMs) in practice. Here I will give an introduction to LLMs and present 3 levels of working with them. Future articles will explore practical aspects of LLMs, such as how to use OpenAI’s public API, the Hugging Face Transformers Python library, how to fine-tune LLMs, and how to build an LLM from scratch."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "What is an LLM?",
        "text": "LLM is short for Large Language Model, which is a recent innovation in AI and machine learning. This powerful new type of AI went viral in Dec 2022 with the release of ChatGPT."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "What is an LLM?",
        "text": "For those enlightened enough to live outside the world of AI buzz and tech news cycles, ChatGPT is a chat interface that ran on an LLM called GPT-3 (now upgraded to either GPT-3.5 or GPT-4 at the time of writing this)."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "What is an LLM?",
        "text": "If you’ve used ChatGPT, it’s obvious that this is not your traditional chatbot from AOL Instant Messenger or your credit card’s customer care."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "What makes an LLM “large”?",
        "text": "When I heard the term “Large Language Model,” one of my first questions was, how is this different from a “regular” language model?"
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "What makes an LLM “large”?",
        "text": "A language model is more generic than a large language model. Just like all squares are rectangles but not all rectangles are squares. All LLMs are language models, but not all language models are LLMs."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "What makes an LLM “large”?",
        "text": "Okay, so LLMs are a special type of language model, but what makes them special?"
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "What makes an LLM “large”?",
        "text": "There are 2 key properties that distinguish LLMs from other language models. One is quantitative, and the other is qualitative."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "What makes an LLM “large”?",
        "text": "Quantitatively, what distinguishes an LLM is the number of parameters used in the model. Current LLMs have on the order of 10–100 billion parameters [1].Qualitatively, something remarkable happens when a language model becomes “large.” It exhibits so-called emergent properties e.g. zero-shot learning [1]. These are properties that seem to suddenly appear when a language model reaches a sufficiently large size."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "Zero-shot Learning",
        "text": "The major innovation of GPT-3 (and other LLMs) is that it is capable of zero-shot learning in a wide variety of contexts [2]. This means ChatGPT can perform a task even if it has not been explicitly trained to do it."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "Zero-shot Learning",
        "text": "While this might be no big deal to us highly evolved humans, this zero-shot learning ability starkly contrasts the prior machine learning paradigm."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "Zero-shot Learning",
        "text": "Previously, a model needed to be explicitly trained on the task it aimed to do in order to have good performance. This could require anywhere from 1k-1M pre-labeled training examples."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "Zero-shot Learning",
        "text": "For instance, if you wanted a computer to do language translation, sentiment analysis, and identify grammatical errors. Each of these tasks would require a specialized model trained on a large set of labeled examples. Now, however, LLMs can do all these things without explicit training."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "How do LLMs work?",
        "text": "The core task used to train most state-of-the-art LLMs is word prediction. In other words, given a sequence of words, what is the probability distribution of the next word?"
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "How do LLMs work?",
        "text": "For example, given the sequence “Listen to your ____,” the most likely next words might be: heart, gut, body, parents, grandma, etc. This might look like the probability distribution shown below."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "How do LLMs work?",
        "text": "Interestingly, this is the same way many (non-large) language models have been trained in the past (e.g. GPT-1) [3]. However, for some reason, when language models get beyond a certain size (say ~10B parameters), these (emergent) abilities, such as zero-shot learning, can start to pop up [1]."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "How do LLMs work?",
        "text": "Although there is no clear answer as to why this occurs (only speculations for now), it is clear that LLMs are a powerful technology with countless potential use cases."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "Now we turn to how to use this powerful technology in practice. While there are countless potential LLM use cases, here I categorize them into 3 levels ordered by required technical knowledge and computational resources. We start with the most accessible."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "The first level of using LLMs in practice is prompt engineering, which I define as any use of an LLM out-of-the-box i.e. not changing any model parameters. While many technically-inclined individuals seem to scoff at the idea of prompt engineering, this is the most accessible way to use LLMs (both technically and economically) in practice."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "There are 2 main ways to do prompt engineering: the Easy Way and the Less Easy Way."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "The Easy Way: ChatGPT (or another convenient LLM UI) — The key benefit of this method is convenience. Tools like ChatGPT provide an intuitive, no-cost, and no-code way to use an LLM (it doesn’t get much easier than that)."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "However, convenience often comes at a cost. In this case, there are 2 key drawbacks to this approach. The first is a lack of functionality. For example, ChatGPT does not readily enable users to customize model input parameters (e.g. temperature or max response length), which are values that modulate LLM outputs. Second, interactions with the ChatGPT UI cannot be readily automated and thus applied to large-scale use cases."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "While these drawbacks may be dealbreakers for some use cases, both can be ameliorated if we take prompt engineering one step further."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "The Less Easy Way: Interact with LLM directly — We can overcome some of the drawbacks of ChatGPT by interacting directly with an LLM via programmatic interfaces. This could be via public APIs (e.g. OpenAI’s API) or running an LLM locally (using libraries like Transformers)."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "While this way of doing prompt engineering is less convenient (since it requires programming knowledge and potential API costs), it provides a customizable, flexible, and scalable way to use LLMs in practice. Future articles in this series will discuss paid and cost-free ways to do this type of prompt engineering."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "Although prompt engineering (as defined here) can handle most potential LLM applications, relying on a generic model, out-of-the-box may result in sub-optimal performance for specific use cases. For these situations, we can go to the next level of using LLMs."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "The second level of using an LLM is model fine-tuning, which I’ll define as taking an existing LLM and tweaking it for a particular use case by training at least 1 (internal) model parameter i.e. weights and biases. For the aficionados out there, this is an example of transfer learning i.e. using some part of an existing model to develop another model."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "Fine-tuning typically consists of 2 steps. Step 1: Obtain a pre-trained LLM. Step 2: Update model parameters for a specific task given (typically 1000s of) high-quality labeled examples."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "The model parameters define the LLM’s internal representation of the input text. Thus, by tweaking these parameters for a particular task, the internal representations become optimized for the fine-tuning task (or at least that’s the idea)."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "This is a powerful approach to model development because a relatively small number of examples and computational resources can produce exceptional model performance."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "The downside, however, is it requires significantly more technical expertise and computational resources than prompt engineering. In a future article, I will attempt to curb this downside by reviewing fine-tuning techniques and sharing example Python code."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "While prompt engineering and model fine-tuning can likely handle 99% of LLM applications, there are cases where one must go even further."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "The third and final way to use an LLM in practice is to build your own. In terms of model parameters, this is where you come up with all the model parameters from scratch."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "An LLM is primarily a product of its training data. Thus, for some applications, it may be necessary to curate custom, high-quality text corpora for model training—for example, a medical research corpus for the development of a clinical application."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "The biggest upside to this approach is you can fully customize the LLM for your particular use case. This is the ultimate flexibility. However, as is often the case, flexibility comes at the cost of convenience."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "Since the key to LLM performance is scale, building an LLM from scratch requires tremendous computational resources and technical expertise. In other words, this isn’t going to be a solo weekend project but a full team working for months, if not years, with a 7–8F budget."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "3 Levels of Using LLMs",
        "text": "Nevertheless, in a future article in this series, we will explore popular techniques for developing LLMs from scratch."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "Conclusion",
        "text": "While there is more than enough hype about LLMs, they are a powerful innovation in AI. Here, I provided a primer on what LLMs are and framed how they can be used in practice. The next article in this series will give a beginner’s guide to OpenAI’s Python API to help jumpstart your next LLM use case."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "Conclusion",
        "text": "👉 More on LLMs: OpenAI API | Hugging Face Transformers | Prompt Engineering | Fine-tuning | Build an LLM | QLoRA | RAG | Text Embeddings"
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "Resources",
        "text": "Connect: My website | Book a call"
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "Resources",
        "text": "Socials: YouTube 🎥 | LinkedIn | Twitter"
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "Resources",
        "text": "[1] Survey of Large Language Models. arXiv:2303.18223 [cs.CL]"
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "Resources",
        "text": "[2] GPT-3 Paper. arXiv:2005.14165 [cs.CL]"
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "Resources",
        "text": "[3] Radford, A., & Narasimhan, K. (2018). Improving Language Understanding by Generative Pre-Training. (GPT-1 Paper)"
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "Resources",
        "text": "By Shaw Talebi on July 13, 2023."
    },
    {
        "article_title": "A Practical Introduction to LLMs",
        "section": "Resources",
        "text": "Exported from Medium on December 2, 2024."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "How to Build an LLM from Scratch",
        "text": "This is the 6th article in a series on using large language models (LLMs) in practice. Previous articles explored how to leverage pre-trained LLMs via prompt engineering and fine-tuning. While these approaches can handle the overwhelming majority of LLM use cases, it may make sense to build an LLM from scratch in some situations. In this article, we will review key aspects of developing a foundation LLM based on the development of models such as GPT-3, Llama, Falcon, and beyond."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "How to Build an LLM from Scratch",
        "text": "Historically (i.e. less than 1 year ago), training large-scale language models (10b+ parameters) was an esoteric activity reserved for AI researchers. However, with all the AI and LLM excitement post-ChatGPT, we now have an environment where businesses and other organizations have an interest in developing their own custom LLMs from scratch [1]. Although this is not necessary (IMO) for >99% of LLM applications, it is still beneficial to understand what it takes to develop these large-scale models and when it makes sense to build them."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "How much does it cost?",
        "text": "Before diving into the technical aspects of LLM development, let’s do some back-of-the-napkin math to get a sense of the financial costs here."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "How much does it cost?",
        "text": "Meta’s Llama 2 models required about 180,000 GPU hours to train its 7b parameter model and 1,700,000 GPU hours to train the 70b model [2]. Taking orders of magnitude here means that a ~10b parameter model can take 100,000 GPU hours to train, and a ~100b parameter takes 1,000,000 GPU hours."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "How much does it cost?",
        "text": "Translating this into commercial cloud computing costs, an Invidia A100 GPU (i.e. what was used to train Llama 2 models) costs around $1–2 per GPU per hour. That means a ~10b parameter model costs about $150,000 to train, and a ~100b parameter model costs ~$1,500,000."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "How much does it cost?",
        "text": "Alternatively, you can buy the GPUs if you don’t want to rent them. The cost of training will then include the price of the A100 GPUs and the marginal energy costs for model training. An A100 is about $10,000 multiplied by 1000 GPUs to form a cluster. The hardware cost is then on the order of $10,000,000. Next, supposing the energy cost to be about $100 per megawatt hour and it requiring about 1,000 megawatt hours to train a 100b parameter model [3]. That comes to a marginal energy cost of about $100,000 per 100b parameter model."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "How much does it cost?",
        "text": "These costs do not include funding a team of ML engineers, data engineers, data scientists, and others needed for model development, which can easily get to $1,000,000 (to get people who know what they are doing)."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "How much does it cost?",
        "text": "Needless to say, training an LLM from scratch is a massive investment (at least for now). Accordingly, there must be a significant potential upside that is not achievable via prompt engineering or fine-tuning existing models to justify the cost for non-research applications."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "4 Key Steps",
        "text": "Now that you’ve realized you do not want to train an LLM from scratch (or maybe you still do, IDK), let’s see what model development consists of. Here, I break the process down into 4 key steps."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "4 Key Steps",
        "text": "Data CurationModel ArchitectureTraining at ScaleEvaluation"
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "4 Key Steps",
        "text": "Although each step has a bottomless depth of technical detail, the discussion here will stay relatively high-level, only highlighting a handful of key details. The reader is referred to the corresponding cited resource for a deeper dive into any aspect."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 1: Data Curation",
        "text": "Machine learning models are a product of their training data, which means the quality of your model is driven by the quality of your data (i.e. “garbage in, garbage out”)."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 1: Data Curation",
        "text": "This presents a major challenge for LLMs due to the tremendous scale of data required. To get a sense of this, here are the training set sizes for a few popular base models."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 1: Data Curation",
        "text": "GPT-3 175b: 0.5T Tokens [4] (T = Trillion)Llama 70b: 2T tokens [2]Falcon 180b: 3.5T [5]"
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 1: Data Curation",
        "text": "This translates to about a trillion words of text i.e. about 1,000,000 novels or 1,000,000,000 news articles. Note: if you are unfamiliar with the term token, check out the explanation in a previous article of this series."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 1: Data Curation",
        "text": "The internet is the most common LLM data mine, which includes countless text sources such as webpages, books, scientific articles, codebases, and conversational data. There are many readily available open datasets for training LLMs such as Common Crawl (and filtered variants Colossal Clean Crawled Corpus (i.e. C4), and Falcon RefinedWeb), The Pile (a cleaned and diverse 825 GB dataset) [6], and many others on Hugging Face’s datasets platform (and elsewhere)."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 1: Data Curation",
        "text": "An alternative to gathering human-generated text from the Internet (and other sources) is to have an existing LLM (e.g. GPT-3) generate a (relatively) high-quality training text corpus. This is what researchers at Stanford did to develop Alpaca, an LLM trained on text generated by GPT-3 with an instruction-input-output format [7]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 1: Data Curation",
        "text": "Regardless of where your text is sourced, diversity is a key aspect of a good training dataset. This tends to improve model generalization for downstream tasks [8]. Most popular foundation models have at least some degree of training data diversity, as illustrated in the figure."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 1: Data Curation",
        "text": "Gathering a mountain of text data is only half the battle. The next stage of data curation is to ensure training data quality. While there are countless ways one can go about this, here I will focus on 4 key text preprocessing steps based on the review by Zhao et al. [8]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 1: Data Curation",
        "text": "Quality Filtering — This aims to remove “low-quality” text from the dataset [8]. This might be non-sensical text from some corner of the web, toxic comments on a news article, extraneous or repeating characters, and beyond. In other words, this is text that does not serve the goals of model development. Zhao et al. split this step into two categories of approaches: classifier-based and heuristic-based. The former involves training a classifier to score the quality of text using a (smaller) high-quality dataset to filter low-quality text. The latter approach employs rules of thumb to ensure data quality e.g. drop high perplexity text, keep only text with particular statistical features, or remove specific words/language[8]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 1: Data Curation",
        "text": "De-duplication — Another key preprocessing step is text de-duplication. This is important because several instances of the same (or very similar) text can bias the language model and disrupt the training process [8]. Additionally, this helps reduce (and ideally eliminate) identical sequences of text present in both the training and testing datasets [9]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 1: Data Curation",
        "text": "Privacy redaction — When scraping text from the internet, there is a risk of capturing sensitive and confidential information. The LLM could then \"learn\" and expose this information unexpectedly. That is why removing personally identifiable information is critical. Both classifier-based and heuristic-based approaches can be used to achieve this."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 1: Data Curation",
        "text": "Tokenization — Language models (i.e. neural networks) do not “understand” text; they can only work with numbers. Thus, before we can train a neural network to do anything, the training data must be translated into numerical form via a process called tokenization. A popular way to do this is via the bytepair encoding (BPE) algorithm [10], which can efficiently translate a given text into numbers by tying particular subwords to particular integers. The main benefit of this approach is it minimizes the number of “out-of-vocabulary” words, which is a problem for other word-based tokenization procedures. The SentencePiece and Tokenizers Python libraries provide implementations of this algorithm [11, 12]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 2: Model Architecture",
        "text": "Transformers have emerged as the state-of-the-art approach for language modeling [13]. While this provides guardrails for model architecture, there are still high-level design decisions that one can make within this framework."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 2: Model Architecture",
        "text": "A transformer is a neural network architecture that uses attention mechanisms to generate mappings between inputs and outputs. An attention mechanism learns dependencies between different elements of a sequence based on its content and position [13]. This comes from the intuition that with language, context matters."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 2: Model Architecture",
        "text": "For example, in the sentence, “I hit the baseball with a bat.” the appearance of the word “baseball” implies that “bat” is a baseball bat and not a nocturnal mammal. However, relying solely on the content of the context isn’t enough. The position and ordering of the words are also important."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 2: Model Architecture",
        "text": "For instance, if we rearrange the same words into, “I hit the bat with a baseball.” This new sentence has an entirely different meaning, and “bat” here is (plausibly) a nocturnal mammal. Note: please do not harm bats."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 2: Model Architecture",
        "text": "Attention allows the neural network to capture the importance of content and position for modeling language. This has been an idea in ML for decades. However, the major innovation of the Transformer’s attention mechanism is computations can be done in parallel, providing significant speed-ups compared to recurrent neural networks, which rely on serial computations [13]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 2: Model Architecture",
        "text": "Transformers consist of 2 key modules: an encoder and a decoder. These modules can be standalone or combined, which enables three types of Transformers [14, 15]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 2: Model Architecture",
        "text": "Encoder-only — an encoder translates tokens into a semantically meaningful numerical representation (i.e. embeddings) using self-attention. Embeddings take context into account. Thus, the same word/token will have different representations depending on the words/tokens around it. These transformers work well for tasks requiring input understanding, such as text classification or sentiment analysis [15]. A popular encoder-only model is Google’s BERT [16]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 2: Model Architecture",
        "text": "Decoder-only — a decoder, like an encoder, translates tokens into a semantically meaningful numerical representation. The key difference, however, is a decoder does not allow self-attention with future elements in a sequence (aka masked self-attention). Another term for this is causal language modeling, implying the asymmetry between future and past tokens. This works well for text generation tasks and is the underlying design of most LLMs (e.g. GPT-3, Llama, Falcon, and many more) [8, 15]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 2: Model Architecture",
        "text": "Encoder-Decoder — we can combine the encoder and decoder modules to create an encoder-decoder transformer. This was the architecture proposed in the original “Attention is all you need” paper [13]. The key feature of this type of transformer (not possible with the other types) is cross-attention. In other words, instead of restricting the attention mechanism to learn dependencies between tokens in the same sequence, cross-attention learns dependencies between tokens in different sequences (i.e. sequences from encoder and decoder modules). This is helpful for generative tasks that require an input, such as translation, summarization, or question-answering [15]. Alternative names for this type of model are masked language model or denoising autoencoder. A popular LLM using this design is Facebook’s BART [17]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 2: Model Architecture",
        "text": "Residual Connections (RC) — (also called skip connections) allow intermediate training values to bypass hidden layers, which tends to improve training stability and performance [14]. One can configure RCs in an LLM in many ways, as discussed in the paper by He et al. (see Figure 4) [18]. The original Transformers paper implements RCs by combining the inputs and outputs of each sublayer (e.g. multi-headed attention layer) via addition and normalization [13]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 2: Model Architecture",
        "text": "Layer Normalization (LN) — is the idea of re-scaling intermediate training values between layers based on their mean and standard deviation (or something similar). This helps speed up training time and makes training more stable [19]. There are two aspects of LN. One is concerned with where you normalize (i.e. pre- or post-layer or both), and the other is how you normalize (e.g. Layer Norm or RMS Norm). The most common approach among LLMs is to apply Pre-LN using the method proposed by Ba et al. [8][19], which differs from the original Transformer architecture, which employed Post-LN [13]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 2: Model Architecture",
        "text": "Activation function (AF) — AFs introduce non-linearities into the model, allowing it to capture complex mappings between input and output. Many common AFs are used for LLMs, including GeLU, ReLU, Swish, SwiGLU, and GeGLU [8]. However, GeLUs are the most common, based on the survey by Zhao et al. [8]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 2: Model Architecture",
        "text": "Position embedding (PE) — PEs capture information about token positions in a language model’s representation of text. One way of doing this is by adding a unique value to each token based on its position in a sequence via sinusoidal functions [13]. Alternatively, one can derive relative positional encodings (RPE) by augmenting a transformer self-attention mechanism to capture distances between sequence elements [20]. The main upside of RPE is performance gains for input sequences much larger than those seen during training [8]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 2: Model Architecture",
        "text": "There is an important balance between training time, dataset size, and model size. If the model is too big or trained too long (relative to the training data), it can overfit. If too small or not trained long enough, it may underperform. Hoffman et al. present an analysis for optimal LLM size based on compute and token count and recommend a scaling schedule including all three factors [21]. Roughly, they recommend 20 tokens per model parameter (i.e. 10B parameters should be trained on 200B tokens) and a 100x increase in FLOPs for each 10x increase in model parameters."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 3: Training at Scale",
        "text": "Large language models (LLMs) are trained via self-supervised learning. What this typically looks like (i.e. in the case of a decoder-only transformer) is predicting the final token in a sequence based on the preceding ones."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 3: Training at Scale",
        "text": "While this is conceptually straightforward, the central challenge emerges in scaling up model training to ~10–100B parameters. To this end, one can employ several common techniques to optimize model training, such as mixed precision training, 3D parallelism, and Zero Redundancy Optimizer (ZeRO)."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 3: Training at Scale",
        "text": "Mixed precision training is a common strategy to reduce the computational cost of model development. This method uses both 32-bit (single precision) and 16-bit (half precision) floating point data types in the training process, such that the use of single precision data is minimized [8, 22]. This helps both decrease memory requirements and shorten training time [22]. While data compression can provide significant improvements in training costs, it can only go so far. This is where parallelization comes into play."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 3: Training at Scale",
        "text": "Parallelization distributes training across multiple computational resources (i.e. CPUs or GPUs or both). Traditionally, this is accomplished by copying model parameters to each GPU so that parameter updates can be done in parallel. However, when training models with hundreds of billions of parameters, memory constraints and communication between GPUs become an issue (e.g. Llama 70b is ~120GB). To mitigate these issues, one can use 3D Parallelism, which combines three parallelization strategies: pipeline, model, and data parallelism."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 3: Training at Scale",
        "text": "Pipeline parallelism — distributes transformer layers across multiple GPUs and reduces the communication volume during distributed training by loading consecutive layers on the same GPU [8].Model parallelism (or tensor parallelism) — decomposes parameter matrix operation into multiple matrix multiplies distributed across multiple GPUs [8].Data parallelism — distributes training data across multiple GPUs. While this requires model parameters and optimizer states to be copied and communicated between GPUs, the downsides are diminished via the preceding parallelization strategies and the next training technique [8]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 3: Training at Scale",
        "text": "While 3D parallelism produces tremendous speed-ups in computation time, there is still a degree of data redundancy when copying model parameters across multiple computational units. This brings up the idea of a Zero Redundancy Optimizer (ZeRO), which (as the name suggests) reduces data redundancy regarding the optimizer state, gradient, or parameter partitioning [8]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 3: Training at Scale",
        "text": "These three training techniques (and many more) are implemented by DeepSpeed, a Python library for deep learning optimization [23]. This has integrations with open-source libraries such as transformers, accelerate, lightning, mosaic ML, determined AI, and MMEngine. Other popular libraries for large-scale model training include Colossal-AI, Alpa, and Megatron-LM."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 3: Training at Scale",
        "text": "Beyond computational costs, scaling up LLM training presents challenges in training stability i.e. the smooth decrease of the training loss toward a minimum value. A few approaches to manage training instability are model checkpointing, weight decay, and gradient clipping."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 3: Training at Scale",
        "text": "Checkpointing — takes a snapshot of model artifacts so training can resume from that point. This is helpful in cases of model collapse (e.g. spike in loss function) because it allows training to be restarted from a point prior to the failure [8].Weight decay — is a regularization strategy that penalizes large parameter values by adding a term (e.g. L2 norm of weights) to the loss function or changing the parameter update rule [24]. A common weight decay value is 0.1 [8].Gradient clipping — rescales the gradient of the objective function if its norm exceeds a pre-specified value. This helps avoid the exploding gradient problem [25]. A common gradient clipping threshold is 1.0 [8]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 3: Training at Scale",
        "text": "Hyperparameters are settings that control model training. While these are not specific to LLMs, a list of key hyperparameters is provided below for completeness."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 3: Training at Scale",
        "text": "Batch size — is the number of samples the optimization will work through before updating parameters [14]. This can either be a fixed number or dynamically adjusted during training. In the case of GPT-3, batch size is increased from 32K to 3.2M tokens [8]. Static batch sizes are typically large values, such as 16M tokens [8].Learning rate — controls the optimization step size. Like batch size, this can also be static or dynamic. However, many LLMs employ a dynamic strategy where the learning rate increases linearly until reaching a maximum value (e.g. 6E-5 for GPT-3) and then reduces via a cosine decay until the learning rate is about 10% of its max value [8].Optimizer — this defines how to update model parameters to reduce the loss. Adam-based optimizers are most commonly used for LLMs [8].Dropout — zeros out a portion of model parameters at random during training. This helps avoid overfitting by, in a sense, training and averaging over a virtual ensemble of models [14]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 3: Training at Scale",
        "text": "Note — Since training an LLM involves tremendous computational expense, it is advantageous to get a sense of the tradeoffs between model size, training time, and performance before training. One way to do this is by estimating these quantities based on predictable scaling laws. The popular work by Kaplan et al. demonstrates how decoder-only model performance scales with parameter count and training time [26]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 4: Evaluation",
        "text": "Successfully training a model is, in many ways, just the beginning. Model development is almost always iterative in that steps are repeated until the developer(s) and stakeholder(s) are satisfied with the final product."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 4: Evaluation",
        "text": "A key part of this iterative process is model evaluation, which examines model performance on a set of tasks [8]. While the task set depends largely on the desired application of the model, there are many benchmarks commonly used to evaluate LLMs."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 4: Evaluation",
        "text": "The Open LLM leaderboard hosted by Hugging Face aims to provide a general ranking of performance for open-access LLMs. The evaluation is based on four benchmark datasets: ARC, HellaSwag, MMLU, and TruthfulQA."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 4: Evaluation",
        "text": "ARC is a question-answering dataset consisting of grade-school level multiple-choice science questions and answers. For example: Which technology was developed most recently? A. Cellular Phone, B. Television, C. Refrigerator, D. Airplane (Answer: A) [27].Hellaswag is a commonsense natural language inference dataset that was specifically constructed to be difficult for machines yet almost trivial for humans via an adversarial filtering strategy. An example task is as follows. A woman is outside with a bucket and a dog. The dog is running around trying to avoid a bath. She… A. rinses the bucket off with soap and blow dry the dog’s head, B. uses a hose to keep it from getting soapy., C. gets the dog wet, then it runs away again., D. gets into a bath tub with the dog. (Answer: C) [28].Massive Multitask Language Understanding (MMLU) evaluates world knowledge and problem-solving ability via 57 tasks spanning grade-school level math, US history, computer science, law, and more [29]. For example: How many numbers are in the list 25, 26, …, 100? A. 75 B. 76 C. 22 D. 23. (Answer: B)TruthfulQA is a benchmark that aims to quantify a language model’s truthfulness by evaluating its responses to common misconceptions [30]. For example, What happens if you crack your knuckles a lot? (False Answer) If you crack your knuckles a lot, you may develop arthritis [30]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 4: Evaluation",
        "text": "For benchmarks that have multiple-choice or categorical targets, model performance can be evaluated using prompt templates. This is demonstrated below, where a question from the ARC dataset is converted into a prompt. We can feed this prompt into our model and compare the highest probability next token (out of “A”, “B”, “C”, and “D”) with the correct answer (i.e. A) [31]."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 4: Evaluation",
        "text": "However, more open-ended tasks are a little more challenging (e.g. TruthfulQA). This is because evaluating the validity of a text output can be much more ambiguous than comparing two discrete classes (i.e. multiple-choice targets)."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 4: Evaluation",
        "text": "One way to overcome this challenge is to evaluate model performance manually via human evaluation. This is where a person scores LLM completions based on a set of guidelines, the ground truth, or both. While this can be cumbersome, it can help foster flexible and high-fidelity model evaluations."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 4: Evaluation",
        "text": "Alternatively, one can take a more quantitative approach and use NLP metrics such as Perplexity, BLEU, or ROGUE scores. While each of these scores is formulated differently, they each quantify the similarity between text generated by the model and the (correct) text in the validation dataset. This is less costly than manual human evaluation but may come at the expense of evaluation fidelity since these metrics are based on statistical properties of generated/ground truth texts and not necessarily their semantic meanings."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Step 4: Evaluation",
        "text": "Finally, an approach that may capture the best of both worlds is to use an auxiliary fine-tuned LLM to compare model generations with the ground truth. One version of this is demonstrated by GPT-judge, a fine-tuned model to classify responses to the TruthfulQA dataset as true or false [30]. However, there is always a risk with this approach since no model can be trusted to have 100% accuracy in all scenarios."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "What’s next?",
        "text": "While we may have only scratched the surface of developing a large language model (LLM) from scratch, I hope this was a helpful primer. For a deeper dive into the aspects mentioned here, check out the references cited below."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "What’s next?",
        "text": "Whether you grab a foundation model off the shelf or build it yourself, it will likely not be very useful. Base models (as the name suggests) are typically a starting place for an AI solution to a problem rather than a final solution. Some applications only require the base model to be used via clever prompts (i.e. prompt engineering), while others warrant fine-tuning the model for a narrow set of tasks. These approaches are discussed in greater detail (with example code) in the previous two articles in this series."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "What’s next?",
        "text": "👉 More on LLMs: Introduction | OpenAI API | Hugging Face Transformers | Prompt Engineering | Fine-tuning | QLoRA | RAG | Text Embeddings"
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Resources",
        "text": "Connect: My website | Book a call | Ask me anything"
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Resources",
        "text": "Socials: YouTube 🎥 | LinkedIn | Twitter"
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Resources",
        "text": "[19] arXiv:1607.06450 [stat.ML]"
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Resources",
        "text": "[22] Trained with Mixed Precision Nvidia Doc"
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Resources",
        "text": "[24] https://paperswithcode.com/method/weight-decay"
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Resources",
        "text": "[25] https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48"
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Resources",
        "text": "[31] https://huggingface.co/blog/evaluating-mmlu-leaderboard"
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Resources",
        "text": "By Shaw Talebi on September 21, 2023."
    },
    {
        "article_title": "How to Build an LLM from Scratch",
        "section": "Resources",
        "text": "Exported from Medium on December 2, 2024."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "LLM Fine-Tuning — FAQs",
        "text": "Last year, I posted an article on fine-tuning large language models (LLMs). To my surprise, this turned out to be one of my most-read blogs ever, and it led to dozens of conversations with clients about their fine-tuning questions and AI projects. Here, I will summarize these conversations' most frequently asked questions and my responses."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "What is Fine-tuning?",
        "text": "I like to define fine-tuning as taking an existing (pre-trained) model and training at least 1 model parameter to adapt it to a particular use case."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "What is Fine-tuning?",
        "text": "It’s important to note the “training at least 1 model parameter” part of the definition. Some will define fine-tuning without this nuance (including me at times). However, this distinguishes fine-tuning from approaches like prompt engineering or prefix-tuning, which adapt a model’s behavior without modifying its internal operations."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "When NOT to Fine-tune",
        "text": "People tend to think that LLM engineering techniques like prompt engineering, RAG, and fine-tuning all live on a spectrum, where fine-tuning is an objectively more powerful version of these other approaches. However, this is misleading."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "When NOT to Fine-tune",
        "text": "The effectiveness of any approach will depend on the details of the use case. For example, fine-tuning is less effective than retrieval augmented generation (RAG) to provide LLMs with specialized knowledge [1]."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "When NOT to Fine-tune",
        "text": "This makes sense when we consider training data volume. For instance, Llama 3.1 8B was trained on ~15T tokens (~1M novels) [2]. A robust fine-tuning dataset might be 1M tokens (1,000 examples consisting of ~1000 tokens each), which is about 10 million times smaller! Thus, any knowledge from a fine-tuning dataset is negligible compared to what was learned at pre-training."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "When do I Fine-tune?",
        "text": "This is not to say that fine-tuning is useless. A central benefit of fine-tuning an AI assistant is lowering inference costs [3]."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "When do I Fine-tune?",
        "text": "The standard way of adapting LLM outputs for a particular application is prompt engineering. In this method, users craft prompts that elicit helpful responses from the model. While this provides a simple and flexible way to adjust model responses, effective prompts may require detailed descriptions of the desired task and several examples the model can mimic, like the one shown below."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "When do I Fine-tune?",
        "text": "Fine-tuning, on the other hand, can compress prompt sizes by directly training the model on examples. Shorter prompts mean fewer tokens at inference, leading to lower compute costs and faster model responses [3]. For instance, after fine-tuning, the above prompt could be compressed to the following."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "RAG vs Fine-tuning?",
        "text": "We’ve already mentioned situations where RAG and fine-tuning perform well. However, since this is such a common question, it’s worth reemphasizing when each approach works best."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "RAG vs Fine-tuning?",
        "text": "RAG is when we inject relevant context into an LLM’s input prompt so that it can generate more helpful responses. For example, if we have a domain-specific knowledge base (e.g., internal company documents and emails), we might identify the items most relevant to the user’s query so that an LLM can synthesize information in an accurate and digestible way."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "RAG vs Fine-tuning?",
        "text": "Here’s high-level guidance on when to use each."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "RAG vs Fine-tuning?",
        "text": "RAG: necessary knowledge for the task is not commonly known or available on the web but can be stored in a databaseFine-tuning: necessary knowledge for the task is already baked into the model, but you want to reduce the prompt size or refine response qualityRAG + Fine-tuning: the task requires specialized knowledge, and we would like to reduce the prompt size or refine the response quality"
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "RAG vs Fine-tuning?",
        "text": "Notice that these approaches are not mutually exclusive. In fact, the original RAG system proposed by Facebook researchers used fine-tuning to better use retrieved information for generating responses [4]."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "How Do I Pick a Model?",
        "text": "Most state-of-the-art language models are trained in similar ways on similar datasets. Thus, performance differences of comparable models are often negligible across use cases."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "How Do I Pick a Model?",
        "text": "However, performance is only one of many considerations important for an AI project. Others include privacy, technical requirements, and cost."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "How Do I Pick a Model?",
        "text": "An open-weight model's parameters are accessible to the public, while the parameters of a closed-weight model are not."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "How Do I Pick a Model?",
        "text": "Open-weight models: Llama (Meta), Mistral, Gemma (Google), Phi (Microsoft)Closed-weight models: GPT 3+ (OpenAI), Claude (Anthropic), Gemini (Google)"
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "How Do I Pick a Model?",
        "text": "The two main considerations when deciding open-weight vs closed-weight are privacy and flexibility. For instance, some projects may have strict controls on where user data can be sent, which may disqualify models accessible only via API."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "How Do I Pick a Model?",
        "text": "On the other hand, some use cases require greater flexibility than those afforded by closed-weight model APIs, such as retraining specific parameters in a model or accessing intermediate model representations."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "How Do I Pick a Model?",
        "text": "Another key question is how big a model should be used. This comes down to a trade-off between model performance and inference costs."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "How Do I Pick a Model?",
        "text": "The approach I’d recommend is to start big to confirm the desired performance is obtainable, then gradually explore smaller and smaller models until performance drops below what’s required. From here, an evaluation can be made on which size choice provides the greatest ROI based on the use case."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "How Do I Pick a Model?",
        "text": "Today’s most popular large language models undergo instruction tuning. In this process, an initial foundation model is fine-tuned to respond to user queries."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "How Do I Pick a Model?",
        "text": "Most business cases I’ve encountered involved an AI chatbot or assistant. For these types of applications, instruction-tuned models are the best choice."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "How Do I Pick a Model?",
        "text": "However, there are situations where directly using foundation models may work better. Namely, if the LLM is not used for question-answering or conversational tasks."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "How to Prepare Data for Fine-tuning?",
        "text": "While much of the attention goes to model leaderboards and fine-tuning approaches, these are secondary to the quality of the training dataset. In other words, the data you use to fine-tune your model will be the key driver of its performance."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "How to Prepare Data for Fine-tuning?",
        "text": "From my experience, most clients are interested in making a “custom chatbot” or “ChatGPT for X.” In these cases, the best fine-tuning approach is so-called supervised fine-tuning. This consists of generating a set of example query-response pairs from which a model can be fine-tuned."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "How to Prepare Data for Fine-tuning?",
        "text": "For example, if I wanted to fine-tune an LLM to respond to viewer questions on YouTube, I would need to gather a set of comments with questions and my associated responses. For a concrete example of this, check out the code walk-through on YouTube."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "Advanced Fine-tuning",
        "text": "Much of this article has focused on fine-tuning large language models to create AI assistants. Despite the popularity of such use cases, this is a relatively narrow application of model fine-tuning."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "Advanced Fine-tuning",
        "text": "Another way we can fine-tune language models is for classification tasks, such as classifying support ticket tiers, detecting spam emails, or determining the sentiment of a customer review. A classic fine-tuning approach for this is called transfer learning, where we replace the head of a language model to perform a new classification task."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "Advanced Fine-tuning",
        "text": "Fine-tuning can also be applied to text embeddings, which are most commonly used today to form vector databases in RAG systems. A major shortcoming of out-the-shelf text embedding models is that they may not perform well on domain-specific language or jargon. Fine-tuning a text embedding model for a particular domain can help overcome this."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "Advanced Fine-tuning",
        "text": "Model compression aims to reduce the size of an LLM without sacrificing performance. While this does not necessarily fit under the fine-tuning definition above, it is in the same spirit of adapting a model for a particular use case. Three popular compression techniques include quantization, pruning, and knowledge distillation."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "What’s Next?",
        "text": "Here, I summarized the most common fine-tuning questions I’ve received over the past 12 months. While fine-tuning is not a panacea for all LLM use cases, it has key benefits."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "What’s Next?",
        "text": "If you have questions that were not covered in the article, drop a comment, and I’ll share my thoughts :)"
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "What’s Next?",
        "text": "My website: https://www.shawhintalebi.com/"
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "What’s Next?",
        "text": "[1] https://arxiv.org/abs/2312.05934"
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "What’s Next?",
        "text": "By Shaw Talebi on September 26, 2024."
    },
    {
        "article_title": "LLM Fine-tuning — FAQs",
        "section": "What’s Next?",
        "text": "Exported from Medium on December 2, 2024."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Compressing Large Language Models (LLMs)",
        "text": "This article is part of a larger series on using large language models (LLMs) in practice. While the immense scale of LLMs is responsible for their impressive performance across a wide range of use cases, this presents challenges in their application to real-world problems. In this article, I discuss how we can overcome these challenges by compressing LLMs. I start with a high-level overview of key concepts and then walk through a concrete example with Python code."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Compressing Large Language Models (LLMs)",
        "text": "The AI mantra of 2023 was \"Bigger is Better,\" where the equation for improving language models was pretty simple: more data + more parameters + more compute = better performance [1]."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Compressing Large Language Models (LLMs)",
        "text": "While this is likely still the case (GPT-5 coming soon?), there are obvious challenges with working with 100B+ parameter models. For example, a 100B parameter model using FP16 requires 200GB just for storage!"
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Compressing Large Language Models (LLMs)",
        "text": "Needless to say, most consumer devices (e.g. phones, tablets, laptops) can’t handle models this big. But.. what if we could make them smaller?"
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Model Compression",
        "text": "Model compression aims to reduce the size of machine learning models without sacrificing performance [2]. This works for (big) neural networks because they are often over-parameterized (i.e. consist of redundant computational units) [3]."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Model Compression",
        "text": "The key benefit of model compression is lower inference costs. This means wider accessibility of powerful ML models (i.e. running LLMs locally on your laptop), lower-cost integration of AI into consumer products, and on-device inference, which supports user privacy and security [3]."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "3 Ways to Compress Models",
        "text": "There is a wide range of techniques for model compression. Here, I will focus on 3 broad categories."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "3 Ways to Compress Models",
        "text": "Quantization — Representing models with lower precision data typesPruning — Removing unnecessary components from a modelKnowledge Distillation — Training a smaller model using a bigger one"
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "3 Ways to Compress Models",
        "text": "Note: these approaches are independent of one another. Thus, techniques from multiple categories can be combined for maximum compression!"
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "1) Quantization",
        "text": "While quantization might sound like a scary and sophisticated word, it is a simple idea. It consists of lowering the precision of model parameters. You can think of this as converting a high-resolution image to a lower-resolution one while still maintaining the picture’s core properties."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "1) Quantization",
        "text": "Two common classes of quantization techniques are Post-training Quantization (PTQ) and Quantization-Aware Training (QAT)."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "1) Quantization",
        "text": "Given a neural network, Post-training Quantization (PTQ) compresses the model by replacing parameters with a lower-precision data type (e.g. FP16 to INT-8). This is one of the fastest and simplest ways to reduce a model’s computational requirements because it requires no additional training or data labeling [4]."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "1) Quantization",
        "text": "While this is a relatively easy way to cut model costs, excessive quantization in this way (e.g., FP16 to INT4) often leads to performance degradation, which limits the potential gains of PTQ. [3]"
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "1) Quantization",
        "text": "For situations where greater compression is needed, PTQ's limitations can be overcome by training models (from scratch) with lower-precision data types. This is the idea behind Quantization-Aware Training (QAT) [5]."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "1) Quantization",
        "text": "While this approach is more technically demanding, it can lead to a significantly smaller, well-performing model. For instance, the BitNet architecture used a ternary data type (i.e. 1.58-bit) to match the performance of the original Llama LLM [6]!"
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "1) Quantization",
        "text": "Of course, a large technical gap exists between PTQ and from-scratch QAT. An approach between the two is Quantization-aware Fine-tuning, which consists of additional training of a pre-trained model after quantization [3]."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "2) Pruning",
        "text": "The goal of pruning is to remove model components that have little impact on performance [7]. This is effective because ML models (especially large ones) tend to learn redundant and noisy structures [3]."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "2) Pruning",
        "text": "An analogy here is like clipping dead branches from a tree. Removing them reduces the size of the tree without harming it."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "2) Pruning",
        "text": "Pruning approaches can be categorized into two buckets: Unstructured and Structured Pruning."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "2) Pruning",
        "text": "Unstructured pruning removes unimportant weights from a neural network (i.e. setting them to zero). For example, early works such as Optimal Brain Damage and Optimal Brain Surgeon computed a saliency score for each parameter in the network by estimating the impact pruning on the loss function [7]."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "2) Pruning",
        "text": "More recently, magnitude-based approaches (i.e. removing weights with the smallest absolute value) have become more popular due to their simplicity and scalability [7]."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "2) Pruning",
        "text": "While the granularity of unstructured pruning can significantly decrease parameter count, these gains require specialized hardware to be realized [7]. Unstructured pruning results in sparse matrix operations (i.e. multiplying matrixes with lots of zeros), which standard hardware cannot do more efficiently than non-sparse operations."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "2) Pruning",
        "text": "Alternatively, structured pruning removes entire structures from a neural network (e.g. attention heads, neurons, and layers) [5]. This avoids the spare matrix operation problem because entire matrices can be dropped from the model rather than individual parameters."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "2) Pruning",
        "text": "While there are various ways to identify structures for pruning, in principle, they all seek to remove structures with the smallest impact on performance. A survey of structured pruning approaches is available in reference [5]."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "3) Knowledge Distillation",
        "text": "Knowledge Distillation transfers knowledge from a (larger) teacher model to a (smaller) student model [5]. One way to do this is to generate predictions with a teacher model and use them to train a student model. Learning from the teacher model’s output logits (i.e., probabilities for all possible next tokens) provides richer information than the original training data, which improves student model performance [8]."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "3) Knowledge Distillation",
        "text": "More recent distillation applications discard the need for logits altogether and learn from synthetic data generated from the teacher model. A popular example is Stanford’s Alpaca model, which fine-tuned the LLaMa 7B (foundation) model using synthetic data from OpenAI’s text-davinci-003 (i.e. the original ChatGPT model), enabling it to follow user instructions [9]."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "With a basic understanding of various compression techniques, let’s see a hands-on example of how to do this in Python. Here, we will compress a 100M parameter model that classifies URLs as safe or unsafe (i.e. phishing)."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "We first use knowledge distillation to compress the 100M parameter model into a 50M parameter one. Then, using 4-bit quantization, we further reduced the memory footprint by 3X, resulting in a final model that is 7X smaller than our original one."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "The example code is available on GitHub. The models (Teacher, Student, Student-4bit) and dataset are freely available on the Hugging Face Hub."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "We start by importing a few helpful libraries."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "Then, we load our dataset from the Hugging Face Hub. This includes training (2100 rows), testing (450 rows), and validation (450 rows) sets."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "Next, we load in our teacher model. To help speed up training, I loaded the model onto a T4 GPU that was freely available on Google Colab."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "The teacher model is a fine-tuned version of Goolge’s bert-base-uncased that performs binary classification on phishing website URLs. The code to train the teacher model is available on GitHub."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "For the student model, we initialize a new model from scratch based on distilbert-base-uncased. We modify the architecture by removing two layers and four attention heads from the remaining layers."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "Before we can train our student model, we will need to tokenize the dataset. This is important because the models expect input text to be represented in a particular way."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "Here, I pad examples based on each batch's longest example. This allows the batches to be represented as a PyTorch tensor."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "Another important step before training is defining an evaluation strategy for our models during training. Below, I define a function that computes the accuracy, precision, recall, and F1 score given a model and dataset."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "Now, we are ready to begin the training process. To allow our student model to learn from both the ground truth labels in the training set (i.e., hard targets) and the teacher model’s logits (i.e., soft targets), we must construct a special loss function that considers both targets."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "This is done by combining the KL divergence of the student and teacher’s output probability distribution with the cross entropy loss of the student’s logits with the ground truth."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "Next, we define our hyperparameters, optimizer, and train/test datasets."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "Finally, we train our student model using PyTorch."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "The training results are shown in the screenshot below. Remarkably, by the end of training, the student model outperformed the teacher across all evaluation metrics!"
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "As a final step, we can evaluate the models on the independent validation set, i.e., data not used in training model parameters or tuning hyperparameters."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "Here, again, we see the student outperform the teacher."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "So far, we’ve reduced our model from 109M parameters (438 MB) to 52.8M parameters (211 MB). However, we can go one step further and quantize the student model."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "First, we push the model of the Hugging Face Hub."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "Then, we can load it back in using 4-bit quantization. For that, we can use the BitsAndBytes integration in the transformers library."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "We set up the config to store model parameters using the 4-bit NormalFloat data type described in the QLoRA paper and the bfloat16 for computation [10]."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "We can then evaluate our quantized model on the validation set."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "Once again, we see a small performance improvement after compression (down to 62.7MB). An intuitive explanation for this is Occam’s Razor principle, which states that simpler models are better."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Example code: Compressing a Text Classifier with Knowledge Distillation + Quantization",
        "text": "In this case, the model may be overparameterized for this binary classification task. Thus, simplifying the model results in better performance."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Recap",
        "text": "While modern large language models (LLMs) demonstrate impressive performance on various tasks, their scale presents challenges in deploying them in real-world settings."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Recap",
        "text": "Recent innovations in model compression techniques help mitigate these challenges by reducing the computational cost of LLM solutions. Here, we discussed three broad categories of compression techniques (Quantization, Pruning, and Knowledge Distillation) and walked through an example implementation in Python."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Recap",
        "text": "My website: https://www.shawhintalebi.com/"
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Recap",
        "text": "[1] Scaling Laws for Neural Language Models"
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Recap",
        "text": "[2] A Survey of Model Compression and Acceleration for Deep Neural Networks"
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Recap",
        "text": "[3] Model Compression in Practice: Lessons Learned from Practitioners Creating On-device Machine Learning Experiences"
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Recap",
        "text": "[4] Model Compression for Deep Neural Networks: A Survey"
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Recap",
        "text": "[5] A Survey on Model Compression for Large Language Models"
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Recap",
        "text": "[6] The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Recap",
        "text": "[7] To prune, or not to prune: exploring the efficacy of pruning for model compression"
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Recap",
        "text": "[8] Distilling the Knowledge in a Neural Network"
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Recap",
        "text": "[9] Alpaca: A Strong, Replicable Instruction-Following Model"
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Recap",
        "text": "[10] QLoRA: Efficient Finetuning of Quantized LLMs"
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Recap",
        "text": "By Shaw Talebi on August 30, 2024."
    },
    {
        "article_title": "Compressing Large Language Models (LLMs)",
        "section": "Recap",
        "text": "Exported from Medium on December 2, 2024."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Cracking Open the Hugging Face Transformers Library",
        "text": "This is the 3rd article in a series on using large language models (LLMs) in practice. Here I will give a beginner-friendly guide to the Hugging Face Transformers library, which provides an easy and cost-free way to work with a wide variety of open-source language models. I will start by reviewing key concepts and then dive into example Python code."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Cracking Open the Hugging Face Transformers Library",
        "text": "In the previous article of this series, we explored the OpenAI Python API and used it to make a custom chatbot. One downside of this API, however, is that API calls cost money, which may not scale well for some use cases."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Cracking Open the Hugging Face Transformers Library",
        "text": "In these scenarios, it may be advantageous to turn to open-source solutions. One popular way to do this is via Hugging Face’s Transformers library."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "What is Hugging Face?",
        "text": "Hugging Face is an AI company that has become a major hub for open-source machine learning (ML). Their platform has 3 major elements which allow users to access and share machine learning resources."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "What is Hugging Face?",
        "text": "First is their rapidly growing repository of pre-trained open-source ML models for things such as natural language processing (NLP), computer vision, and more. Second is their library of datasets for training ML models for almost any task. Third, and finally, is Spaces which is a collection of open-source ML apps hosted by Hugging Face."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "What is Hugging Face?",
        "text": "The power of these resources is that they are community generated, which leverages all the benefits of open-source (i.e. cost-free, wide diversity of tools, high-quality resources, and rapid pace of innovation). While these make building powerful ML projects more accessible than before, there is another key element of the Hugging Face ecosystem — the Transformers library."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "🤗Transformers",
        "text": "Transformers is a Python library that makes downloading and training state-of-the-art ML models easy. Although it was initially made for developing language models, its functionality has expanded to include models for computer vision, audio processing, and beyond."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "🤗Transformers",
        "text": "Two big strengths of this library are, one, it easily integrates with Hugging Face’s (previously mentioned) Models, Datasets, and Spaces repositories, and two, the library supports other popular ML frameworks such as PyTorch and TensorFlow."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "🤗Transformers",
        "text": "This results in a simple and flexible all-in-one platform for downloading, training, and deploying machine learning models and apps."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "🤗Transformers",
        "text": "The easiest way to start using the library is via the pipeline() function, which abstracts NLP (and other) tasks into 1 line of code. For example, if we want to do sentiment analysis, we would need to select a model, tokenize the input text, pass it through the model, and decode the numerical output to determine the sentiment label (positive or negative)."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "🤗Transformers",
        "text": "While this may seem like a lot of steps, we can do all this in 1 line via the pipeline() function, as shown in the code snippet below."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "🤗Transformers",
        "text": "Of course, sentiment analysis is not the only thing we can do here. Almost any NLP task can be done in this way e.g. summarization, translation, question-answering, feature extraction (i.e. text embedding), text generation, zero-shot-classification, and more — for a full list of the built-in tasks, check out the pipleine() documentation."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "🤗Transformers",
        "text": "In the above example code, since we did not specify a model, the default model for sentiment analysis was used (i.e. distilbert-base-uncased-finetuned-sst-2-english). However, if we wanted to be more explicit, we could have used the following line of code."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "🤗Transformers",
        "text": "One of the greatest benefits of the Transformers library is we could have just as easily used any of the 28,000+ text classification models on Hugging Face’s Models repository by simply changing the model name passed into the pipeline() function."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "🤗Transformers",
        "text": "There is a massive repository of pre-trained models available on Hugging Face (277,528 at the time of writing this). Almost all these models can be easily used via Transformers, using the same syntax we saw in the above code block."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "🤗Transformers",
        "text": "However, the models on Hugging Face aren’t only for the Transformers library. There are models for other popular machine learning frameworks e.g. PyTorch, Tensorflow, Jax. This makes Hugging Face’s Models repository useful to ML practitioners beyond the context of the Transformers library."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "🤗Transformers",
        "text": "To see what navigating the repository looks like, let’s consider an example. Say we want a model that can do text generation, but we want it to be available via the Transformers library so we can use it in one line of code (as we did above). We can easily view all models that fit these criteria using the “Tasks” and “Libraries” filters."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "🤗Transformers",
        "text": "A model that meets these criteria is the newly released Llama 2. More specifically, Llama-2–7b-chat-hf, which is a model in the Llama 2 family with about 7 billion parameters, optimized for chat, and in the Hugging Face Transformers format. We can get more information about this model via its model card, which is shown in the figure below."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Installing 🤗Transformers (with Conda)",
        "text": "Now that we have a basic idea of the resources offered by Hugging Face and the Transformers library let’s see how we can use them. We start by installing the library and other dependencies."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Installing 🤗Transformers (with Conda)",
        "text": "Hugging Face provides an installation guide on its website. So, I won’t try to (poorly) duplicate that guide here. However, I will provide a quick 2-step guide on how to set up the conda environment for the example code below."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Installing 🤗Transformers (with Conda)",
        "text": "Step 1) The first step is to download the hf-env.yml file available at the GitHub repository. You can either download the file directly or clone the whole repo."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Installing 🤗Transformers (with Conda)",
        "text": "Step 2) Next, in your terminal (or anaconda command prompt), you can create a new conda environment based on hf-env.yml using the following commands"
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Installing 🤗Transformers (with Conda)",
        "text": "This may take a couple of minutes to install, but once it’s complete, you should be ready to go!"
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Example Code: NLP with 🤗Transformers",
        "text": "With the necessary libraries installed, let’s jump into some example code. Here we will survey 3 NLP use cases, namely, sentiment analysis, summarization, and conversational text generation, using the pipeline() function."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Example Code: NLP with 🤗Transformers",
        "text": "Toward the end, we will use Gradio to quickly generate a User Interface (UI) for any of these use cases and deploy it as an app on Hugging Face Spaces. All example code is available on the GitHub repository."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Example Code: NLP with 🤗Transformers",
        "text": "We start sentiment analysis. Recall from earlier when we used the pipeline function to do something like the code block below, where we create a classifier that can label the input text as being either positive or negative."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Example Code: NLP with 🤗Transformers",
        "text": "To go one step further, instead of processing text one by one, we can pass a list to the classifier to process as a batch."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Example Code: NLP with 🤗Transformers",
        "text": "However, the text classification models on Hugging Face are not limited to just positive-negative sentiment. For example, the “roberta-base-go_emotions” model by SamLowe generates a suite of class labels. We can just as easily apply this model to text, as shown in the code snippet below."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Example Code: NLP with 🤗Transformers",
        "text": "Another way we can use the pipeline() function is for text summarization. Although this is an entirely different task than sentiment analysis, the syntax is almost identical."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Example Code: NLP with 🤗Transformers",
        "text": "We first load in a summarization model. Then pass in some text along with a couple of input parameters."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Example Code: NLP with 🤗Transformers",
        "text": "For more sophisticated use cases, it may be necessary to use multiple models in succession. For example, we can apply sentiment analysis to the summarized text to speed up the runtime."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Example Code: NLP with 🤗Transformers",
        "text": "Finally, we can use models developed specifically to generate conversational text. Since conversations require past prompts and responses to be passed to subsequent model responses, the syntax is a little different here. However, we start by instantiating our model using the pipeline() function."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Example Code: NLP with 🤗Transformers",
        "text": "Next, we can use the Conversation() class to handle the back-and-forth. We initialize it with a user prompt, then pass it into the chatbot model from the previous code block."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Example Code: NLP with 🤗Transformers",
        "text": "To keep the conversation going, we can use the add_user_input() method to add another prompt to the conversation. We then pass the conversation object back into the chatbot."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Example Code: NLP with 🤗Transformers",
        "text": "While we get the base chatbot functionality with the Transformer library, this is an inconvenient way to interact with a chatbot. To make the interaction a bit more intuitive, we can use Gradio to spin up a front end in a few lines of Python code."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Example Code: NLP with 🤗Transformers",
        "text": "This is done with the code shown below. At the top, we initialize two lists to store user messages and model responses, respectively. Then we define a function that will take the user prompt and generate a chatbot output. Next, we create the chat UI using the Gradio ChatInterface() class. Finally, we launch the app."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Example Code: NLP with 🤗Transformers",
        "text": "This will spin up the UI via a local URL. If the window does not open automatically, you can copy and paste the URL directly into your browser."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Example Code: NLP with 🤗Transformers",
        "text": "To go one step further, we can quickly deploy this UI via Hugging Face Spaces. These are Git repositories hosted by Hugging Face and augmented by computational resources. Both free and paid options are available depending on the use case. Here we will stick with the free option."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Example Code: NLP with 🤗Transformers",
        "text": "To make a new Space, we first go to the Spaces page and click “Create new space”. Then, configure the Space by giving it the name e.g. “my-first-space” and selecting Gradio as the SDK. Then hit “Create Space”."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Example Code: NLP with 🤗Transformers",
        "text": "Next, we need to upload app.py and requirements.txt files to the Space. The app.py file houses the code we used to generate the Gradio UI, and the requirements.txt file specifies the app’s dependencies. The files for this example are available at the GitHub repo and the Hugging Face Space."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Example Code: NLP with 🤗Transformers",
        "text": "Finally, we push the code to the Space just like we would to GitHub. The end result is a public application hosted on Hugging Face Spaces."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Example Code: NLP with 🤗Transformers",
        "text": "App link: https://huggingface.co/spaces/shawhin/my-first-space"
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Conclusion",
        "text": "Hugging Face has become synonymous with open-source language models and machine learning. The biggest advantage of their ecosystem is it gives small-time developers, researchers, and tinkers access to powerful ML resources."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Conclusion",
        "text": "While we covered a lot of material in this post, we’ve only scratched the surface of what the Hugging Face ecosystem can do. In future articles of this series, we will explore more advanced use cases and cover how to fine-tune models using 🤗Transformers."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Conclusion",
        "text": "👉 More on LLMs: Introduction | OpenAI API | Prompt Engineering | Fine-tuning | Build an LLM | QLoRA | RAG | Text Embeddings"
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Resources",
        "text": "Connect: My website | Book a call | Ask me anything"
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Resources",
        "text": "Socials: YouTube 🎥 | LinkedIn | Twitter"
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Resources",
        "text": "[1] Hugging Face — https://huggingface.co/"
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Resources",
        "text": "[2] Hugging Face Course — https://huggingface.co/learn/nlp-course/chapter1/1"
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Resources",
        "text": "By Shaw Talebi on August 5, 2023."
    },
    {
        "article_title": "Cracking Open the Hugging Face Transformers Library",
        "section": "Resources",
        "text": "Exported from Medium on December 2, 2024."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "text": "This article is part of a larger series on using large language models (LLMs) in practice. In the previous post, we saw how to fine-tune an LLM using OpenAI. The main limitation to this approach, however, is that OpenAI’s models are concealed behind their API, which limits what and how we can build with them. Here, I’ll discuss an alternative way to fine-tune an LLM using open-source models and QLoRA."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "text": "Fine-tuning is when we take an existing model and tweak it for a particular use case. This has been a critical part of the recent explosion of AI innovations, giving rise to ChatGPT and the like."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "text": "Although fine-tuning is a simple (and powerful) idea, applying it to LLMs isn’t always straightforward. The key challenge is that LLMs are (very) computationally expensive (i.e. they aren’t something that can be trained on a typical laptop)."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "text": "For example, standard fine-tuning of a 70B parameter model requires over 1TB of memory [1]. For context, an A100 GPU comes with up to 80GB of memory, so you’d (at best) need over a dozen of these $20,000 cards!"
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "text": "While this may deflate your dreams of building a custom AI, don’t give up just yet. The open-source community has been working hard to make building with these models more accessible. One popular method that has sprouted from these efforts is QLoRA (Quantized Low-Rank Adaptation), an efficient way to fine-tune a model without sacrificing performance."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "What is Quantization?",
        "text": "A key part of QLoRA is so-called quantization. While this might sound like a scary and sophisticated word, it is a simple idea. When you hear “quantizing,” think of splitting a range of numbers into buckets."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "What is Quantization?",
        "text": "For example, there are infinite possible numbers between 0 and 100, e.g. 1, 12, 27, 55.3, 83.7823, and so on. We could quantize this range by splitting them into buckets based on whole numbers so that (1, 12, 27, 55.3, 83.7823) becomes (1, 12, 27, 55, 83), or we could use factors of ten so that the numbers become (0, 0, 20, 50, 80). A visualization of this process is shown below."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "What is Quantization?",
        "text": "Quantization allows us to represent a given set of numbers with less information. To see why this is important, let’s (briefly) talk about how computers work."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "What is Quantization?",
        "text": "Computers encode information using binary digits (i.e. bits). For instance, if I want a computer to remember the number 83.7823, this number needs to be translated into a string of 1s and 0s (aka a bit string)."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "What is Quantization?",
        "text": "One way of doing this is via the single-precision floating-point format (i.e. FP32), which represents numbers as a sequence of 32 bits [2]. For example, 83.7823 can be represented as 01000010101001111001000010001010 [3]."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "What is Quantization?",
        "text": "Since a string of 32 bits has 2³² (= 4,294,967,296) unique combinations that means we can represent 4,294,967,296 unique values with FP32. Thus, if we have numbers from 0 to 100, the bit count sets the precision for representing numbers in that range."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "What is Quantization?",
        "text": "But there is another side of the story. If we use 32 bits to represent each model parameter, each parameter will take up 4 bytes of memory (1 byte = 8 bits). Therefore, a 10B parameter model will consume 40 GB of memory. And if we want to do full parameter fine-tuning, that will require closer to 200GB of memory! [1]"
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "What is Quantization?",
        "text": "This presents a dilemma for fine-tuning LLMs. Namely, we want high precision for successful model training, but we need to use as little memory as possible to ensure we don’t run out of it. Balancing this tradeoff is a key contribution of QLoRA."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "QLoRA (or Quantized Low-Rank Adaptation) combines 4 ingredients to get the most out of a machine’s limited memory without sacrificing model performance. I will briefly summarize key points from each. More details are available in the QLoRA paper [4]."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "This first ingredient takes the idea of quantization near its practical limits. In contrast to the typical 16-bit data type (i.e., half-precision floating point) used for language model parameters, QLoRA uses a special data type called 4-bit NormalFloat."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "As the name suggests, this data type encodes numbers with just 4 bits. While this means we only have 2⁴ (= 16) buckets to represent model parameters, 4-bit NormalFloat uses a special trick to get more out of the limited information capacity."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "The naive way to quantize a set of numbers is what we saw earlier, where we split the numbers into equally-spaced buckets. However, a more efficient way would be to use equally-sized buckets. The difference between these two approaches is illustrated in the figure below."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "More specifically, 4-bit NormalFloat employs an information-theoretically optimal quantization strategy for normally distributed data [4]. Since model parameters tend to clump around 0, this is an effective strategy for representing LLM parameters."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "Despite the unfortunate name, double quantization generates memory savings by quantizing the quantization constants (see what I mean)."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "To break this down, consider the following quantization process. Given an FP32 tensor, a simple way to quantize it is using the mathematical formula below [4]."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "Here we are converting the FP32 representation into an Int8 (8-bit integer) representation within the range of [-127, 127]. Notice this boils down to rescaling the values in the tensor X^(FP32) and then rounding them to the nearest integer. We can then simplify the equation by defining a scaling term (or quantization constant) c^FP32 = 127/absmax(X^FP32))."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "While this naive quantization approach isn’t how it’s done in practice (remember the trick we saw with 4-bit NormalFloat), it does illustrate that the quantization comes with some computational overhead to store the resulting constants in memory."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "We could minimize this overhead by doing this process just once. In other words, compute one quantization constant for all the model parameters. However, this is not ideal since it is (very) sensitive to extreme values. In other words, one relatively large parameter value will skew all the others because of the absmax() function in c^FP32."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "Alternatively, we could partition the model parameters into smaller blocks for quantization. This reduces the chances that a large value will skew other values but comes with a larger memory footprint."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "To mitigate this memory cost, we can (again) employ quantization, but now on the constants generated from this block-wise approach. For a block size of 64, an FP32 quantization constant adds 0.5 bits/parameter. By quantizing these constants further, to say 8-bit, we can reduce this footprint to 0.127 bits/parameter [4]."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "This ingredient uses Nvidia’s unified memory feature to help avoid out-of-memory errors during training. It transfers “pages” of memory from the GPU to the CPU when the GPU hits its limits. This is similar to how memory is handled between CPU RAM and machine storage [4]."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "More specifically, this memory paging feature moves pages of optimizer states to the CPU and back to the GPU as needed. This is important because there can be intermittent memory spikes during training, which can kill the process."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "LoRA (Low-rank Adaptation) is a Parameter Efficient Fine-tuning (PEFT) method. The key idea is instead of retraining all the model parameters, LoRA adds a relatively small number of trainable parameters while keeping the original parameters fixed [5]."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "Since I covered the details of LoRA in a previous article of this series, I will just say we can use it to reduce the number of trainable parameters by 100–1000X without sacrificing model performance."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "Now that we know all the ingredients of QLoRA, let’s see how we can bring them together."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "To start, consider a standard fine-tuning process, which consists of retraining every model parameter. What this might look like is using FP16 for the model parameters and gradients (4 total bytes/parameters) and FP32 for the optimizer states, e.g. momentum and variance, and parameters (12 bytes/parameter) [1]. So, a 10B parameter model would require about 160GB of memory to fine-tune."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "Using LoRA, we can immediately reduce this computational cost by decreasing the number of trainable parameters. This works by freezing the original parameters and adding a set of (small) adapters housing the trainable parameters [5]. The computational cost for the model parameters and gradients would be the same as before (4 total bytes/parameters) [1]."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "The savings, however, comes from the optimizer states. If we have 100X fewer trainable parameters and use FP16 for the adapter, we’d have an additional 0.04 bytes per parameter in the original model (as opposed to 4 bytes/parameter). Similarly, using FP32 for the optimizer states, we have an additional 0.12 bytes/parameter [4]. Therefore, a 10B parameter model would require about 41.6GB of memory to fine-tune. A significant savings, but still a lot to ask for from consumer hardware."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "QLoRA takes things further by quantizing the original model parameters using Ingredients 1 and 2. This reduces the cost from 4 bytes/parameter to about 1 byte/parameter. Then, by using LoRA in the same way as before, that would add another 0.16 bytes/parameter. Thus, a 10B model can be fine-tuned with just 11.6GB of memory! This can easily run on consumer hardware like the free T4 GPU on Google Colab."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "QLoRA",
        "text": "A visual comparison of the 3 approaches is shown below [4]."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "Now that we have a basic understanding of how QLoRA works let’s see what using it looks like in code. Here, we will use a 4-bit version of the Mistral-7B-Instruct model provided by TheBloke and the Hugging Face ecosystem for fine-tuning."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "This example code is available in a Google Colab notebook, which can run on the (free) GPU provided by Colab. The dataset is also available on Hugging Face."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "🔗 Google Colab | Training Dataset | GitHub Repo"
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "We import modules from Hugging Face’s transforms, peft, and datasets libraries."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "Additionally, we need the following dependencies installed for some of the previous modules to work."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "Next, we load the quantized model from Hugging Face. Here, we use a version of Mistral-7B-Instruct-v0.2 prepared by TheBloke, who has freely quantized and shared thousands of LLMs."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "Notice we are using the “Instruct” version of Mistral-7b. This indicates that the model has undergone instruction tuning, a fine-tuning process that aims to improve model performance in answering questions and responding to user prompts."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "Other than specifying the model repo we want to download, we also set the following arguments: device_map, trust_remote_code, and revision. device_map lets the method automatically figure out how to best allocate computational resources for loading the model on the machine. Next, trust_remote_code=False prevents custom model files from running on your machine. Then, finally, revision specifies which version of the model we want to use from the repo."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "Once loaded, we see the 7B parameter model only takes us 4.16GB of memory, which can easily fit in either the CPU or GPU memory available for free on Colab."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "Next, we load the tokenizer for the model. This is necessary because the model expects the text to be encoded in a specific way. I discussed tokenization in previous articles of this series."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "Next, we can use the model for text generation. As a first pass, let’s try to input a test comment to the model. We can do this in 3 steps."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "First, we craft the prompt in the proper format. Namely, Mistral-7b-Instruct expects input text to start and end with the special tokens [INST] and [/INST], respectively. Second, we tokenize the prompt. Third, we pass the prompt into the model to generate text."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "The code to do this is shown below with the test comment, “Great content, thank you!”"
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "The response from the model is shown below. While it gets off to a good start, the response seems to continue for no good reason and doesn’t sound like something I would say."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "This is where prompt engineering is helpful. Since a previous article in this series covered this topic in-depth, I’ll just say that prompt engineering involves crafting instructions that lead to better model responses."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "Typically, writing good instructions is something done through trial and error. To do this, I tried several prompt iterations using together.ai, which has a free UI for many open-source LLMs, such as Mistral-7B-Instruct-v0.2."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "Once I got instructions I was happy with, I created a prompt template that automatically combines these instructions with a comment using a lambda function. The code for this is shown below."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "We can see the power of a good prompt by comparing the new model response (below) to the previous one. Here, the model responds concisely and appropriately and identifies itself as ShawGPT."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "Let’s see how we can improve the model’s performance through fine-tuning. We can start by enabling gradient checkpointing and quantized training. Gradient checkpointing is a memory-saving technique that clears specific activations and recomputes them during the backward pass [6]. Quantized training is enabled using the method imported from peft."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "Next, we can set up training with LoRA via a configuration object. Here, we target the query layers in the model and use an intrinsic rank of 8. Using this config, we can create a version of the model that can undergo fine-tuning with LoRA. Printing the number of trainable parameters, we observe a more than 100X reduction."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "Now, we can import our training data. The dataset used here is available on the HuggingFace Dataset Hub. I generated this dataset using comments and responses from my YouTube channel. The code to prepare and upload the dataset to the Hub is available at the GitHub repo."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "Next, we must prepare the dataset for training. This involves ensuring examples are an appropriate length and are tokenized. The code for this is shown below."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "Two other things we need for training are a pad token and a data collator. Since not all examples are the same length, a pad token can be added to examples as needed to make it a particular size. A data collator will dynamically pad examples during training to ensure all examples in a given batch have the same length."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "In the code block below, I define hyperparameters for model training."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "While several are listed here, the two I want to highlight in the context of QLoRA are fp16 and optim. fp16=True has the trainer use FP16 values for the training process, which results in significant memory savings compared to the standard FP32. optim=”paged_adamw_8bit” enables Ingredient 3 (i.e. paged optimizers) discussed previously."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "With all the hyperparameters set, we can run the training process using the code below."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "Since we only have 50 training examples, the process runs in about 10 minutes. The training and validation loss are shown in the table below. We can see that both losses monotonically decrease, indicating stable training."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "The final model is freely available on the HF hub. If you want to skip the training process and load it directly, you can use the following code."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "We can use the fine-tuned model for inference in the same way as before. Here is the fine-tuned model’s response to the same test comment as before (i.e. “Great content, thank you!”)."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "The response is much more concise and even adds a disclaimer that it is an AI. If we want to remove this disclaimer, we can easily do that using basic string manipulation in Python."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "As another example, let’s try a more technical test comment: “What is fat-tailedness?” The model’s response is given below."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "The response is similar to what we saw in the previous article of this series with the fine-tuned OpenAI model. It gives a concise and appropriate explanation of fat-tailedness, but this isn’t how I explain fat-tailedness."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "While we could attempt to capture this specialized knowledge via further fine-tuning, a simpler approach would be to augment the fine-tuned model using external knowledge from my article series on fat tails (and other data science topics)."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Example Code: Fine-tuning Mistral-7b-Instruct to respond to YouTube comments",
        "text": "This brings up the idea of Retrieval Augmented Generation (i.e. RAG), which will be discussed in the next article of this series."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "What’s Next?",
        "text": "QLoRA is a fine-tuning technique that has made building custom large language models more accessible. Here, I gave an overview of how the approach works and shared a concrete example of using QLoRA to create a YouTube comment responder."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "What’s Next?",
        "text": "While the fine-tuned model did a qualitatively good job mimicking my response style, it had some limitations in understanding specialized data science knowledge. In the next article of this series, we will see how we can overcome this limitation by improving the model with RAG."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Resources",
        "text": "Connect: My website | Book a call"
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Resources",
        "text": "Socials: YouTube 🎥 | LinkedIn | Instagram"
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Resources",
        "text": "[1] Memory Optimization with ZeRO"
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Resources",
        "text": "[4] QLoRA: Efficient Finetuning of Quantized LLMs"
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Resources",
        "text": "By Shaw Talebi on February 22, 2024."
    },
    {
        "article_title": "QLoRA — How to Fine-Tune an LLM on a Single GPU",
        "section": "Resources",
        "text": "Exported from Medium on December 2, 2024."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Multimodal Embeddings: An Introduction",
        "text": "This is the 2nd article in a larger series on multimodal AI. In the previous post, we saw how to augment large language models (LLMs) to understand new data modalities (e.g., images, audio, video). One such approach relied on encoders that generate vector representations (i.e. embeddings) of non-text data. In this article, I will discuss multimodal embeddings and share what they can do via two practical use cases."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Multimodal Embeddings: An Introduction",
        "text": "AI research is traditionally split into distinct fields: NLP, computer vision (CV), robotics, human-computer interface (HCI), etc. However, countless practical tasks require the integration of these different research areas e.g. autonomous vehicles (CV + robotics), AI agents (NLP + CV + HCI), personalized learning (NLP + HCI), etc."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Multimodal Embeddings: An Introduction",
        "text": "Although these fields aim to solve different problems and work with different data types, they all share a fundamental process. Namely, generating useful numerical representations of real-world phenomena."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Multimodal Embeddings: An Introduction",
        "text": "Historically, this was done by hand. This means that researchers and practitioners would use their (or other people’s) expertise to explicitly transform data into a more helpful form. Today, however, these can be derived another way."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Embeddings",
        "text": "Embeddings are (useful) numerical representations of data learned implicitly through model training. For example, through learning how to predict text, BERT learned representations of text, which are helpful for many NLP tasks [1]. Another example is the Vision Transformer (ViT), trained for image classification on Image Net, which can be repurposed for other applications [2]."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Embeddings",
        "text": "A key point here is that these learned embedding spaces will have some underlying structure so that similar concepts are located close together. As shown in the toy examples below."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Embeddings",
        "text": "One key limitation of the previously mentioned models is they are restricted to a single data modality, e.g., text or images. Preventing cross-modal applications like image captioning, content moderation, image search, and more. But what if we could merge these two representations?"
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Multimodal Embeddings",
        "text": "Although text and images may look very different to us, in a neural network, these are represented via the same mathematical object, i.e., a vector. Therefore, in principle, text, images, or any other data modality can processed by a single model."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Multimodal Embeddings",
        "text": "This fact underlies multimodal embeddings, which represent multiple data modalities in the same vector space such that similar concepts are co-located (independent of their original representations)."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Multimodal Embeddings",
        "text": "For example, CLIP encodes text and images into a shared embedding space [3]. A key insight from CLIP is that by aligning text and image representations, the model is capable of 0-shot image classification on an arbitrary set of target classes since any input text can be treated as a class label (we will see a concrete example of this later)."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Multimodal Embeddings",
        "text": "However, this idea is not limited to text and images. Virtually any data modalities can be aligned in this way e.g., text-audio, audio-image, text-EEG, image-tabular, and text-video. Unlocking use cases such as video captioning, advanced OCR, audio transcription, video search, and EEG-to-text [4]."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Contrastive Learning",
        "text": "The standard approach to aligning disparate embedding spaces is contrastive learning (CL). A key intuition of CL is to represent different views of the same information similarly [5]."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Contrastive Learning",
        "text": "This consists of learning representations that maximize the similarity between positive pairs and minimize the similarity of negative pairs. In the case of an image-text model, a positive pair might be an image with an appropriate caption, while a negative pair would be an image with an irrelevant caption (as shown below)."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Contrastive Learning",
        "text": "Two key aspects of CL contribute to its effectiveness"
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Contrastive Learning",
        "text": "Since positive and negative pairs can be curated from the data’s inherent structure (e.g., metadata from web images), CL training data do not require manual labeling, which unlocks larger-scale training and more powerful representations [3].It simultaneously maximizes positive and minimizes negative pair similarity via a special loss function, as demonstrated by CLIP [3]."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image search",
        "text": "With a high-level understanding of how multimodal embeddings work, let’s see two concrete examples of what they can do. Here, I will use the open-source CLIP model to perform two tasks: 0-shot image classification and image search."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image search",
        "text": "The code for these examples is freely available on the GitHub repository."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image search",
        "text": "The basic idea behind using CLIP for 0-shot image classification is to pass an image into the model along with a set of possible class labels. Then, a classification can be made by evaluating which text input is most similar to the input image."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image search",
        "text": "We’ll start by importing the Hugging Face Transformers library so that the CLIP model can be downloaded locally. Additionally, the PIL library is used to load images in Python."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image search",
        "text": "Next, we can import a version of the clip model and its associated data processor. Note: the processor handles tokenizing input text and image preparation."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image search",
        "text": "We load in the below image of a cat and create a list of two possible class labels: “a photo of a cat” or “a photo of a dog”."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image search",
        "text": "Next, we’ll preprocess the image/text inputs and pass them into the model."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image search",
        "text": "To make a class prediction, we must extract the image logits and evaluate which class corresponds to the maximum."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image search",
        "text": "The model nailed it with a 99.79% probability that it’s a cat photo. However, this was a super easy one. Let’s see what happens when we change the class labels to: “ugly cat” and “cute cat” for the same image."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image search",
        "text": "The model easily identified that the image was indeed a cute cat. Let’s do something more challenging like the labels: “cat meme” or “not cat meme”."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image search",
        "text": "While the model is less confident about this prediction with a 54.64% probability, it correctly implies that the image is not a meme."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image search",
        "text": "Another application of CLIP is essentially the inverse of Use Case 1. Rather than identifying which text label matches an input image, we can evaluate which image (in a set) best matches a text input (i.e. query)—in other words, performing a search over images."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image search",
        "text": "We start by storing a set of images in a list. Here, I have three images of a cat, dog, and goat, respectively."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image search",
        "text": "Next, we can define a query like “a cute dog” and pass it and the images into CLIP."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image search",
        "text": "We can then match the best image to the input text by extracting the text logits and evaluating the image corresponding to the maximum."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image search",
        "text": "We see that (again) the model nailed this simple example. But let’s try some trickier examples."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "Example Code: Using CLIP for 0-shot classification and image search",
        "text": "Although this last prediction is quite controversial, all the other matches were spot on! This is likely since images like these are ubiquitous on the internet and thus were seen many times in CLIP’s pre-training."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "What’s Next?",
        "text": "Multimodal embeddings unlock countless AI use cases that involve multiple data modalities. Here, we saw two such use cases, i.e., 0-shot image classification and image search using CLIP."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "What’s Next?",
        "text": "Another practical application of models like CLIP is multimodal RAG, which consists of the automated retrieval of multimodal context to an LLM. In the next article of this series, we will see how this works under the hood and review a concrete example."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "What’s Next?",
        "text": "My website: https://www.shawhintalebi.com/"
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "What’s Next?",
        "text": "[1] BERT[2] ViT[3] CLIP[4] Thought2Text: Text Generation from EEG Signal using Large Language Models (LLMs)[5] A Simple Framework for Contrastive Learning of Visual Representations"
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "What’s Next?",
        "text": "By Shaw Talebi on November 29, 2024."
    },
    {
        "article_title": "Multimodal Embeddings: An Introduction",
        "section": "What’s Next?",
        "text": "Exported from Medium on December 2, 2024."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Fine-Tuning Large Language Models (LLMs)",
        "text": "This is the 5th article in a series on using Large Language Models (LLMs) in practice. In this post, we will discuss how to fine-tune (FT) a pre-trained LLM. We start by introducing key FT concepts and techniques, then finish with a concrete example of how to fine-tune a model (locally) using Python and Hugging Face’s software ecosystem."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Fine-Tuning Large Language Models (LLMs)",
        "text": "In the previous article of this series, we saw how we could build practical LLM-powered applications by integrating prompt engineering into our Python code. For the vast majority of LLM use cases, this is the initial approach I recommend because it requires significantly less resources and technical expertise than other methods while still providing much of the upside."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Fine-Tuning Large Language Models (LLMs)",
        "text": "However, there are situations where prompting an existing LLM out-of-the-box doesn’t cut it, and a more sophisticated solution is required. This is where model fine-tuning can help."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "What is Fine-tuning?",
        "text": "Fine-tuning is taking a pre-trained model and training at least one internal model parameter (i.e. weights). In the context of LLMs, what this typically accomplishes is transforming a general-purpose base model (e.g. GPT-3) into a specialized model for a particular use case (e.g. ChatGPT) [1]."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "What is Fine-tuning?",
        "text": "The key upside of this approach is that models can achieve better performance while requiring (far) fewer manually labeled examples compared to models that solely rely on supervised training."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "What is Fine-tuning?",
        "text": "While strictly self-supervised base models can exhibit impressive performance on a wide variety of tasks with the help of prompt engineering [2], they are still word predictors and may generate completions that are not entirely helpful or accurate. For example, let’s compare the completions of davinci (base GPT-3 model) and text-davinci-003 (a fine-tuned model)."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "What is Fine-tuning?",
        "text": "Notice the base model is simply trying to complete the text by listing a set of questions like a Google search or homework assignment, while the fine-tuned model gives a more helpful response. The flavor of fine-tuning used for text-davinci-003 is alignment tuning, which aims to make the LLM’s responses more helpful, honest, and harmless, but more on that later [3,4]."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Why Fine-tune",
        "text": "Fine-tuning not only improves the performance of a base model, but a smaller (fine-tuned) model can often outperform larger (more expensive) models on the set of tasks on which it was trained [4]. This was demonstrated by OpenAI with their first generation “InstructGPT” models, where the 1.3B parameter InstructGPT model completions were preferred over the 175B parameter GPT-3 base model despite being 100x smaller [4]."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Why Fine-tune",
        "text": "Although most of the LLMs we may interact with these days are not strictly self-supervised models like GPT-3, there are still drawbacks to prompting an existing fine-tuned model for a specific use case."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Why Fine-tune",
        "text": "A big one is LLMs have a finite context window. Thus, the model may perform sub-optimally on tasks that require a large knowledge base or domain-specific information [1]. Fine-tuned models can avoid this issue by “learning” this information during the fine-tuning process. This also precludes the need to jam-pack prompts with additional context and thus can result in lower inference costs."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Ways to Fine-tune",
        "text": "There are 3 generic ways one can fine-tune a model: self-supervised, supervised, and reinforcement learning. These are not mutually exclusive in that any combination of these three approaches can be used in succession to fine-tune a single model."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Ways to Fine-tune",
        "text": "Self-supervised learning consists of training a model based on the inherent structure of the training data. In the context of LLMs, what this typically looks like is given a sequence of words (or tokens, to be more precise), predict the next word (token)."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Ways to Fine-tune",
        "text": "While this is how many pre-trained language models are developed these days, it can also be used for model fine-tuning. A potential use case of this is developing a model that can mimic a person’s writing style given a set of example texts."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Ways to Fine-tune",
        "text": "The next, and perhaps most popular, way to fine-tune a model is via supervised learning. This involves training a model on input-output pairs for a particular task. An example is instruction tuning, which aims to improve model performance in answering questions or responding to user prompts [1,3]."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Ways to Fine-tune",
        "text": "The key step in supervised learning is curating a training dataset. A simple way to do this is to create question-answer pairs and integrate them into a prompt template [1,3]. For example, the question-answer pair: Who was the 35th President of the United States? — John F. Kennedy could be pasted into the below prompt template. More example prompt templates are available in section A.2.1 of ref [4]."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Ways to Fine-tune",
        "text": "Using a prompt template is important because base models like GPT-3 are essentially “document completers”. Meaning, given some text, the model generates more text that (statistically) makes sense in that context. This goes back to the previous blog of this series and the idea of “tricking” a language model into solving your problem via prompt engineering."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Ways to Fine-tune",
        "text": "Finally, one can use reinforcement learning (RL) to fine-tune models. RL uses a reward model to guide the training of the base model. This can take many different forms, but the basic idea is to train the reward model to score language model completions such that they reflect the preferences of human labelers [3,4]. The reward model can then be combined with a reinforcement learning algorithm (e.g. Proximal Policy Optimization (PPO)) to fine-tune the pre-trained model."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Ways to Fine-tune",
        "text": "An example of how RL can be used for model fine-tuning is demonstrated by OpenAI’s InstructGPT models, which were developed through 3 key steps [4]."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Ways to Fine-tune",
        "text": "Generate high-quality prompt-response pairs and fine-tune a pre-trained model using supervised learning. (~13k training prompts) Note: One can (alternatively) skip to step 2 with the pre-trained model [3].Use the fine-tuned model to generate completions and have human-labelers rank responses based on their preferences. Use these preferences to train the reward model. (~33k training prompts)Use the reward model and an RL algorithm (e.g. PPO) to fine-tune the model further. (~31k training prompts)"
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Ways to Fine-tune",
        "text": "While the strategy above does generally result in LLM completions that are significantly more preferable to the base model, it can also come at a cost of lower performance in a subset of tasks. This drop in performance is also known as an alignment tax [3,4]."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Supervised Fine-tuning Steps (High-level)",
        "text": "As we saw above, there are many ways in which one can fine-tune an existing language model. However, for the remainder of this article, we will focus on fine-tuning via supervised learning. Below is a high-level procedure for supervised model fine-tuning [1]."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Supervised Fine-tuning Steps (High-level)",
        "text": "Choose fine-tuning task (e.g. summarization, question answering, text classification)Prepare training dataset i.e. create (100–10k) input-output pairs and preprocess data (i.e. tokenize, truncate, and pad text).Choose a base model (experiment with different models and choose one that performs best on the desired task).Fine-tune model via supervised learningEvaluate model performance"
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Supervised Fine-tuning Steps (High-level)",
        "text": "While each of these steps could be an article of their own, I want to focus on step 4 and discuss how we can go about training the fine-tuned model."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Options for Parameter Training",
        "text": "When it comes to fine-tuning a model with ~100M-100B parameters, one needs to be thoughtful of computational costs. Toward this end, an important question is — which parameters do we (re)train?"
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Options for Parameter Training",
        "text": "With the mountain of parameters at play, we have countless choices for which ones we train. Here, I will focus on three generic options of which to choose."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Options for Parameter Training",
        "text": "The first option is to train all internal model parameters (called full parameter tuning) [3]. While this option is simple (conceptually), it is the most computationally expensive. Additionally, a known issue with full parameter tuning is the phenomenon of catastrophic forgetting. This is where the model “forgets” useful information it “learned” in its initial training [3]."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Options for Parameter Training",
        "text": "One way we can mitigate the downsides of Option 1 is to freeze a large portion of the model parameters, which brings us to Option 2."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Options for Parameter Training",
        "text": "The big idea with transfer learning (TL) is to preserve the useful representations/features the model has learned from past training when applying the model to a new task. This generally consists of dropping “the head” of a neural network (NN) and replacing it with a new one (e.g. adding new layers with randomized weights). Note: The head of an NN includes its final layers, which translate the model’s internal representations to output values."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Options for Parameter Training",
        "text": "While leaving the majority of parameters untouched mitigates the huge computational cost of training an LLM, TL may not necessarily resolve the problem of catastrophic forgetting. To better handle both of these issues, we can turn to a different set of approaches."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Options for Parameter Training",
        "text": "PEFT involves augmenting a base model with a relatively small number of trainable parameters. The key result of this is a fine-tuning methodology that demonstrates comparable performance to full parameter tuning at a tiny fraction of the computational and storage cost [5]."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Options for Parameter Training",
        "text": "PEFT encapsulates a family of techniques, one of which is the popular LoRA (Low-Rank Adaptation) method [6]. The basic idea behind LoRA is to pick a subset of layers in an existing model and modify their weights according to the following equation."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Options for Parameter Training",
        "text": "Where h() = a hidden layer that will be tuned, x = the input to h(), W₀ = the original weight matrix for the h, and ΔW = a matrix of trainable parameters injected into h. ΔW is decomposed according to ΔW=BA, where ΔW is a d by k matrix, B is d by r, and A is r by k. r is the assumed “intrinsic rank” of ΔW (which can be as small as 1 or 2) [6]."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Options for Parameter Training",
        "text": "Sorry for all the math, but the key point is the (d * k) weights in W₀ are frozen and, thus, not included in optimization. Instead, the ((d * r) + (r * k)) weights making up matrices B and A are the only ones that are trained."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Options for Parameter Training",
        "text": "Plugging in some made-up numbers for d=100, k=100, and r=2 to get a sense of the efficiency gains, the number of trainable parameters drops from 10,000 to 400 in that layer. In practice, the authors of the LoRA paper cited a 10,000x reduction in parameter checkpoint size using LoRA fine-tune GPT-3 compared to full parameter tuning [6]."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "3 Options for Parameter Training",
        "text": "To make this more concrete, let’s see how we can use LoRA to fine-tune a language model efficiently enough to run on a personal computer."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Example Code: Fine-tuning an LLM using LoRA",
        "text": "In this example, we will use the Hugging Face ecosystem to fine-tune a language model to classify text as ‘positive’ or ‘negative’. Here, we fine-tune distilbert-base-uncased, a ~70M parameter model based on BERT. Since this base model was trained to do language modeling and not classification, we employ transfer learning to replace the base model head with a classification head. Additionally, we use LoRA to fine-tune the model efficiently enough that it can run on my Mac Mini (M1 chip with 16GB memory) in a reasonable amount of time (~20 min)."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Example Code: Fine-tuning an LLM using LoRA",
        "text": "The code, along with the conda environment files, are available on the GitHub repository. The final model and dataset [7] are available on Hugging Face."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Example Code: Fine-tuning an LLM using LoRA",
        "text": "We start by importing helpful libraries and modules. Datasets, transformers, peft, and evaluate are all libraries from Hugging Face (HF)."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Example Code: Fine-tuning an LLM using LoRA",
        "text": "Next, we load in our base model. The base model here is a relatively small one, but there are several other (larger) ones that we could have used (e.g. roberta-base, llama2, gpt2). A full list is available here."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Example Code: Fine-tuning an LLM using LoRA",
        "text": "We can then load our training and validation data from HF’s datasets library. This is a dataset of 2000 movie reviews (1000 for training and 1000 for validation) with binary labels indicating whether the review is positive (or not)."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Example Code: Fine-tuning an LLM using LoRA",
        "text": "Next, we need to preprocess our data so that it can be used for training. This consists of using a tokenizer to convert the text into an integer representation understood by the base model."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Example Code: Fine-tuning an LLM using LoRA",
        "text": "To apply the tokenizer to the dataset, we use the .map() method. This takes in a custom function that specifies how the text should be preprocessed. In this case, that function is called tokenize_function(). In addition to translating text to integers, this function truncates integer sequences such that they are no longer than 512 numbers to conform to the base model’s max input length."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Example Code: Fine-tuning an LLM using LoRA",
        "text": "At this point, we can also create a data collator, which will dynamically pad examples in each batch during training such that they all have the same length. This is computationally more efficient than padding all examples to be equal in length across the entire dataset."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Example Code: Fine-tuning an LLM using LoRA",
        "text": "We can define how we want to evaluate our fine-tuned model via a custom function. Here, we define the compute_metrics() function to compute the model’s accuracy."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Example Code: Fine-tuning an LLM using LoRA",
        "text": "Before training our model, we can evaluate how the base model with a randomly initialized classification head performs on some example inputs."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Example Code: Fine-tuning an LLM using LoRA",
        "text": "As expected, the model performance is equivalent to random guessing. Let’s see how we can improve this with fine-tuning."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Example Code: Fine-tuning an LLM using LoRA",
        "text": "To use LoRA for fine-tuning, we first need a config file. This sets all the parameters for the LoRA algorithm. See comments in the code block for more details."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Example Code: Fine-tuning an LLM using LoRA",
        "text": "We can then create a new version of our model that can be trained via PEFT. Notice that the scale of trainable parameters was reduced by about 100x."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Example Code: Fine-tuning an LLM using LoRA",
        "text": "Next, we define hyperparameters for model training."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Example Code: Fine-tuning an LLM using LoRA",
        "text": "Finally, we create a trainer() object and fine-tune the model!"
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Example Code: Fine-tuning an LLM using LoRA",
        "text": "The above code will generate the following table of metrics during training."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Example Code: Fine-tuning an LLM using LoRA",
        "text": "To see how the model performance has improved, let’s apply it to the same 5 examples from before."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Example Code: Fine-tuning an LLM using LoRA",
        "text": "The fine-tuned model improved significantly from its prior random guessing, correctly classifying all but one of the examples in the above code. This aligns with the ~90% accuracy metric we saw during training."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Example Code: Fine-tuning an LLM using LoRA",
        "text": "Links: Code Repo | Model | Dataset"
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Conclusions",
        "text": "While fine-tuning an existing model requires more computational resources and technical expertise than using one out-of-the-box, (smaller) fine-tuned models can outperform (larger) pre-trained base models for a particular use case, even when employing clever prompt engineering strategies. Furthermore, with all the open-source LLM resources available, it’s never been easier to fine-tune a model for a custom application."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Conclusions",
        "text": "The next article of this series will go one step beyond model fine-tuning and discuss how to train a language model from scratch."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Conclusions",
        "text": "👉 More on LLMs: Introduction | OpenAI API | Hugging Face Transformers | Prompt Engineering | Build an LLM | QLoRA | RAG | Text Embeddings"
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Resources",
        "text": "Connect: My website | Book a call | Ask me anything"
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Resources",
        "text": "Socials: YouTube 🎥 | LinkedIn | Twitter"
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Resources",
        "text": "[1] Deeplearning.ai Finetuning Large Langauge Models Short Course: https://www.deeplearning.ai/short-courses/finetuning-large-language-models/"
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Resources",
        "text": "[2] arXiv:2005.14165 [cs.CL] (GPT-3 Paper)"
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Resources",
        "text": "[3] arXiv:2303.18223 [cs.CL] (Survey of LLMs)"
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Resources",
        "text": "[4] arXiv:2203.02155 [cs.CL] (InstructGPT paper)"
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Resources",
        "text": "[5] 🤗 PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware: https://huggingface.co/blog/peft"
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Resources",
        "text": "[6] arXiv:2106.09685 [cs.CL] (LoRA paper)"
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Resources",
        "text": "[7] Original dataset source — Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning Word Vectors for Sentiment Analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA. Association for Computational Linguistics."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Resources",
        "text": "By Shaw Talebi on September 11, 2023."
    },
    {
        "article_title": "Fine-Tuning Large Language Models (LLMs)",
        "section": "Resources",
        "text": "Exported from Medium on December 2, 2024."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "Cracking Open the OpenAI (Python) API",
        "text": "This is the 2nd article in a series on using Large Language Models (LLMs) in practice. Here I present a beginner-friendly introduction to the OpenAI API. This allows you to go beyond restrictive chat interfaces like ChatGPT and to get more out of LLMs for your unique use cases. Python example code is provided below and at the GitHub repository."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "Cracking Open the OpenAI (Python) API",
        "text": "What’s an API?OpenAI’s (Python) APIGetting Started (4 Steps)Example Code"
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "Cracking Open the OpenAI (Python) API",
        "text": "In the first article of this series, I described Prompt Engineering as the most accessible way to use LLMs in practice. The easiest (and most popular) way to do this is via tools like ChatGPT, which provide an intuitive, no-cost, and no-code way to interact with an LLM."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "Cracking Open the OpenAI (Python) API",
        "text": "However, this ease of use comes at a cost. Namely, the chat UI is restrictive and does not translate well to many practical use cases e.g. building your own customer support bot, real-time sentiment analysis of customer reviews, etc."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "Cracking Open the OpenAI (Python) API",
        "text": "In these cases, we can take Prompt Engineering one step further and interact with LLMs programmatically. One way we can do this is via an API."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "1) What’s an API?",
        "text": "An application programming interface (API) allows you to interact with a remote application programmatically. While this might sound technical and scary, the idea is super simple. Consider the following analogy."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "1) What’s an API?",
        "text": "Imagine you have an intense craving for the pupusas you ate during that summer in El Salvador. Unfortunately, you’re back at home and don’t know where to find good Salvadoran food. Lucky for you, however, you have a super-foodie friend that knows every restaurant in town."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "1) What’s an API?",
        "text": "So, you send your friend the text."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "1) What’s an API?",
        "text": "Then, a couple of minutes later, you get the response."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "1) What’s an API?",
        "text": "While this may seem irrelevant to APIs, this is essentially how they work. You send a request to a remote application i.e. text your super-foodie friend. Then, the remote application sends back a response i.e. the text back from your friend."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "1) What’s an API?",
        "text": "The difference between an API and the above analogy is instead of sending the request with your phone’s texting app, you use your favorite programming language e.g. Python, JavaScript, Ruby, Java, etc. This is great if you are developing software where some external information is required because the information retrieval can be automated."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "2) OpenAI’s (Python) API",
        "text": "We can use APIs to interact with Large Language Models. A popular one is OpenAI’s API, where instead of typing prompts into the ChatGPT web interface, you can send them to and from OpenAI using Python."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "2) OpenAI’s (Python) API",
        "text": "This gives virtually anyone access to state-of-the-art LLMs (and other ML models) without having to provision the computational resources needed to run them. The downside, of course, is OpenAI doesn’t do this as a charity. Each API call costs money, but more on that in a bit."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "2) OpenAI’s (Python) API",
        "text": "Some notable features of the API (not available with ChatGPT) are listed below."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "2) OpenAI’s (Python) API",
        "text": "Customizable system message (this is set to something like “I am ChatGPT, a large language model trained by OpenAI, based on the GPT-3.5 architecture. My knowledge is based on information available up until September 2021. Today’s date is July 13, 2023.” for ChatGPT)Adjust input parameters such as maximum response length, number of responses, and temperature (i.e. the “randomness” of the response).Include images and other file types in promptsExtract helpful word embeddings for downstream tasksInput audio for transcription or translationModel fine-tuning functionality"
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "2) OpenAI’s (Python) API",
        "text": "The OpenAI API has several models from which to choose. The best model to pick will depend on your particular use case. Below is a list of the current models available [1]."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "2) OpenAI’s (Python) API",
        "text": "Note: Each item listed above is accompanied by a set of models which vary in size and cost. Check documentation for the most recent information."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "2) OpenAI’s (Python) API",
        "text": "While the OpenAI API gives developers easy access to SOTA ML models, one obvious downside is that it costs money. Pricing is done on a per-token basis (no, I don’t mean NFTs or something you use at the arcade)."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "2) OpenAI’s (Python) API",
        "text": "Tokens, in the context of LLMs, are essentially a set of numbers representing a set of words and characters. For example, “The” could be a token, “ end” (with the space) could be another, and “.” another."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "2) OpenAI’s (Python) API",
        "text": "Thus, the text “The End.” would consist of 3 tokens say (73, 102, 6)."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "2) OpenAI’s (Python) API",
        "text": "This is a critical step because LLMs (i.e. neural networks) do not “understand” text directly. The text must be converted into a numerical representation so that the model can perform mathematical operations on the input. Hence, the tokenization step."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "2) OpenAI’s (Python) API",
        "text": "The price of an API call depends on the number of tokens used in the prompt and the model being prompted. The price per model is available on OpenAI’s website."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "3) Getting Started (4 Steps)",
        "text": "Now that we have a basic understanding of the OpenAI API let’s see how to use it. Before we can start coding, we need to set up four things."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "3) Getting Started (4 Steps)",
        "text": "To make an account go to the OpenAI API Overview page, and click “Sign Up” in the top right cornerNote — If you’ve used ChatGPT before, then you probably already have an OpenAI account. If so, click “Log in”"
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "3) Getting Started (4 Steps)",
        "text": "If your account is more than 3 months old or the free $5 API credit is not enough for you, you will need to add a payment method before making API calls.Click your profile image and select the manage account option.Then add a payment method by clicking the “Billing” tab and then “Payment methods”."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "3) Getting Started (4 Steps)",
        "text": "Next, I recommend setting usage limits so that you avoid being billed more than you budget for.To do this, go to the “Usage limits” under the “Billing” tab. Here you can set a “Soft” and “Hard” limit.If you hit your monthly soft limit, OpenAI will send you an email notification.If you hit your hard limit, any additional API requests will be denied (thus, you won’t be charged more than this)."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "3) Getting Started (4 Steps)",
        "text": "Click on “View API keys”If this is your first time, you will need to make a new secret key. To do this, click “Create new secret key”Next, you can give your key a custom name. Here I used “my-first-key”.Then, click “Create secret key”"
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "With all the setup done, we are (finally) ready to make our first API call. Here we will use the openai Python library, which makes integrating OpenAI’s models into your Python code super easy. You can download the package via pip. The below example code (and bonus code) is available on the GitHub repo for this article."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "A quick note on Completions API deprecations — OpenAI is moving away from the freeform prompt paradigm and toward chat-based API calls. According to a blog from OpenAI, the chat-based paradigm provides better responses, given its structured prompt interface, compared to the previous paradigm [2]."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "While older OpenAI (GPT-3) models are still available via the “freeform” paradigm, the more recent (and powerful) models (i.e. GPT-3.5-turbo and GPT-4) are only available via chat-based calls."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "Let’s start with a super simple API call. Here we will pass two inputs into the openai.ChatCompletions.create() method i.e. model and messages."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "model — defines the name of the language model we want to use (we can choose from the models listed earlier in the article.)messages — sets the “preceding” chat dialogue as a list of dictionaries. The dictionaries have two key-value pairs (e.g. {“role”: “user”, “content”: “Listen to your”}.) First, “role” defines who is talking (e.g. “role”:”user”). This can either be the “user”, “assistant”, or “system”. Second, “content” defines what the role is saying (e.g. “content”: “Listen to your”). While this may feel more restrictive than a freeform prompt interface, we can get creative with input messages to optimize responses for a particular use case (more on this later)."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "This is what our first API call looks like in Python."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "The API response is stored in the chat_completion variable. Printing chat_completion, we see that it is like a dictionary consisting of 6 key-value pairs."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "The meaning of each field is listed below."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "‘Id’ = unique ID for the API response‘Object’ = name of API object that sent the response‘Created’ = unix timestamp of when the API request was processed‘Model’ = name of the model used‘Choices’ = model response formatted in JSON (i.e. dictionary-like)‘Usage’ = token count meta-data formatted in JSON (i.e. dictionary-like)"
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "However, the main thing we care about here is the ‘Choices’ field since this is where the model response is stored. In this case, we see the “assistant” role responds with the message “heart.”"
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "Yay! We made our 1st API call. Now let’s start playing with the model input parameters."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "First, we can set the maximum number of tokens allowed in the model response using the max_tokens input parameter. This can be helpful for many reasons depending on the use case. In this case, I just want a one-word response, so I’ll set it to 1 token."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "Next, we can set the number of responses we would like to receive from the model. Again, this can be helpful for many reasons depending on the use case. For example, if we want to generate a set of responses from which we can select the one we like best."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "Notice that not all the completions are identical. This may be a good thing or a bad thing based on the use case (e.g. creative use cases vs. process automation use cases). Therefore, it can be advantageous to adjust the diversity of chat completions for a given prompt."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "It turns out we can do this by tuning the temperature parameter. Put simply, this adjusts the “randomness” of chat completions. Values for this parameter range from 0 to 2, where 0 makes completions more predictable, and 2 makes them less predictable [3]."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "Conceptually, we can think of temp=0 will default to the most likely next word while temp=2 will enable completions that are relatively unlikely. Let’s see what this looks like."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "As expected, when temp=0, all 5 completions are identical and produce something “very likely.” Now let’s see what happens when we turn up the temperature."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "Again, as expected, the chat completions with temp=2 were much more diverse and “out of pocket.”"
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "Finally, we can leverage the different roles in this chat-based prompting paradigm to adjust the language model responses even further."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "Recall from earlier that we can include content from 3 different roles in our prompts: system, user, and assistant. The system message sets the context (or task) for model completions e.g. “You are a friendly chatbot that does not want to destroy all humans” or “Summarize user prompts in max 10 words”."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "User and assistant messages can be used in at least two ways. One, to generate examples for in-context learning, and two, to store and update conversation history for a real-time chatbot. Here we will use both ways to create a lyric completion assistant."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "We start by making the system message “I am Roxette lyric completion assistant. When given a line from a song, I will provide the next line in the song.” Then, provide two examples of user and assistant messages. Followed by the same user prompt used in the preceding examples i.e.“Listen to your”."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "Here’s what that looks like in code."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "Comparing the output to the actual lyrics to the hit Roxette song, we see they are an exact match. This is due to the combination of all the different inputs we provided to the model."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "4) Example Code: Chat Completion API",
        "text": "To see what this looks like when we “crank the temperature,” check out the bonus code on GitHub. (Warning: it gets weird)"
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "Conclusion",
        "text": "Here I gave a beginner-friendly guide to the OpenAI Python API with example code. The biggest upside of using OpenAI’s API is you can work with powerful LLMs without worrying about provisioning computational resources. The downsides, however, are API calls cost money and potential security concerns of sharing some types of data with a 3rd party (OpenAI)."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "Conclusion",
        "text": "To avoid these downsides, we can turn to open-source LLM solutions. This will be the focus of the next article in this series, where we’ll explore the Hugging Face Transformers library."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "Conclusion",
        "text": "👉 More on LLMs: Introduction | Hugging Face Transformers | Prompt Engineering | Fine-tuning | Build an LLM | QLoRA | RAG | Text Embeddings"
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "Resources",
        "text": "Connect: My website | Book a call | Ask me anything"
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "Resources",
        "text": "Socials: YouTube 🎥 | LinkedIn | Twitter"
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "Resources",
        "text": "[1] OpenAI Models documentation"
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "Resources",
        "text": "[2] GPT-4 Availability & Completions API Deprecation"
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "Resources",
        "text": "[3] Temperature definition from API reference"
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "Resources",
        "text": "By Shaw Talebi on July 21, 2023."
    },
    {
        "article_title": "Cracking Open the OpenAI (Python) API",
        "section": "Resources",
        "text": "Exported from Medium on December 2, 2024."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Prompt Engineering: How to Trick AI into Solving Your Problems",
        "text": "This is the fourth article in a series on using large language models (LLMs) in practice. Here, I will discuss prompt engineering (PE) and how to use it to build LLM-enabled applications. I start by reviewing key PE techniques and then walk through Python example code of using LangChain to build an LLM-based application."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Prompt Engineering: How to Trick AI into Solving Your Problems",
        "text": "When first hearing about prompt engineering, many technical people (including myself) tend to scoff at the idea. We might think, “Prompt engineering? Psssh, that’s lame. Tell me how to build an LLM from scratch.”"
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Prompt Engineering: How to Trick AI into Solving Your Problems",
        "text": "However, after diving into it more deeply, I’d caution developers against writing off prompt engineering automatically. I’ll go even further and say that prompt engineering can realize 80% of the value of most LLM use cases with (relatively) very low effort."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Prompt Engineering: How to Trick AI into Solving Your Problems",
        "text": "My goal with this article is to convey this point via a practical review of prompt engineering and illustrative examples. While there are surely gaps in what prompt engineering can do, it opens the door to discovering simple and clever solutions to our problems."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "What is Prompt Engineering?",
        "text": "In the first article of this series, I defined prompt engineering as any use of an LLM out-of-the-box (i.e. not training any internal model parameters). However, there is much more that can be said about it."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "What is Prompt Engineering?",
        "text": "Prompt Engineering is “the means by which LLMs are programmed with prompts.” [1]Prompt Engineering is “an empirical art of composing and formatting the prompt to maximize a model’s performance on a desired task.” [2]“language models… want to complete documents, and so you can trick them into performing tasks just by arranging fake documents.” [3]"
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "What is Prompt Engineering?",
        "text": "The first definition conveys the key innovation coming from LLMs, which is that computers can now be programmed using plain English. The second point frames prompt engineering as a largely empirical endeavor, where practitioners, tinkerers, and builders are the key explorers of this new way of programming."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "What is Prompt Engineering?",
        "text": "The third point (from Andrej Karpathy) reminds us that LLMs aren’t explicitly trained to do almost anything we ask them to do. Thus, in some sense, we are “tricking” these language models to solve problems. I feel this captures the essence of prompt engineering, which relies less on your technical skills and more on your creativity."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "2 Levels of Prompt Engineering",
        "text": "There are two distinct ways in which one can do prompt engineering, which I called the “easy way” and the “less easy way” in the first article of this series."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "2 Levels of Prompt Engineering",
        "text": "This is how most of the world does prompt engineering, which is via ChatGPT (or something similar). It is an intuitive, no-code, and cost-free way to interact with an LLM."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "2 Levels of Prompt Engineering",
        "text": "While this is a great approach for something quick and simple, e.g. summarizing a page of text, rewriting an email, helping you brainstorm birthday party plans, etc., it has its downsides. A big one is that it’s not easy to integrate this approach into a larger automated process or software system. To do this, we need to go one step further."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "2 Levels of Prompt Engineering",
        "text": "This resolves many of the drawbacks of the “easy way” by interacting with LLMs programmatically i.e. using Python. We got a sense of how we can do this in the previous two articles of this series, where explored OpenAI’s Python API and the Hugging Face Transformers library."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "2 Levels of Prompt Engineering",
        "text": "While this requires more technical knowledge, this is where the real power of prompt engineering lies because it allows developers to integrate LLM-based modules into larger software systems."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "2 Levels of Prompt Engineering",
        "text": "A good (and perhaps ironic) example of this is ChatGPT. The core of this product is prompting a pre-trained model (i.e. GPT-3.5-turbo) to act like a chatbot and then wrapping it in an easy-to-use web interface."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "2 Levels of Prompt Engineering",
        "text": "Of course, developing GPT-3.5-turbo is the hard part, but that’s not something we need to worry about here. With all the pre-trained LLMs we have at our fingertips, almost anyone with basic programming skills can create a powerful AI application like ChatGPT without being an AI researcher or a machine learning Ph.D."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Building AI Apps with Prompt Engineering",
        "text": "The less easy way unlocks a new paradigm of programming and software development. No longer are developers required to define every inch of logic in their software systems. They now have the option to offload a non-trivial portion to LLMs. Let’s look at a concrete example of what this might look like."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Building AI Apps with Prompt Engineering",
        "text": "Suppose you want to create an automatic grader for a high school history class. The trouble, however, is that all the questions have written responses, so there often can be multiple versions of a correct answer. For example, the following responses to “Who was the 35th president of the United States of America?” could be correct."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Building AI Apps with Prompt Engineering",
        "text": "John F. KennedyJFKJack Kennedy (a common nickname)John Fitzgerald Kennedy (probably trying to get extra credit)John F. Kenedy (misspelled last name)"
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Building AI Apps with Prompt Engineering",
        "text": "In the traditional programming paradigm, it was on the developer to figure out how to account for all these variations. To do this, they might list all possible correct answers and use an exact string-matching algorithm or maybe even use fuzzy matching to help with misspelled words."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Building AI Apps with Prompt Engineering",
        "text": "However, with this new LLM-enabled paradigm, the problem can be solved through simple prompt engineering. For instance, we could use the following prompt to evaluate student answers."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Building AI Apps with Prompt Engineering",
        "text": "We can think of this prompt as a function, where given a question, correct_answer, and student_answer, it generates the student's grade. This can then be integrated into a larger piece of software that implements the automatic grader."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Building AI Apps with Prompt Engineering",
        "text": "In terms of time-saving, this prompt took me about 2 minutes to write, while if I were to try to develop an algorithm to do the same thing, it would take me hours (if not days) and probably have worse performance. So the time savings for tasks like this are 100–1000x."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Building AI Apps with Prompt Engineering",
        "text": "Of course, there are many tasks in which LLMs do not provide any substantial benefit, and other existing methods are much better suited (e.g. predicting tomorrow’s weather). In no way are LLMs the solution to every problem, but they do create a new set of solutions to tasks that require processing natural language effectively—something that has been historically difficult for computers to do."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "7 Tricks for Prompt Engineering",
        "text": "While the prompt example from before may seem like a natural and obvious way to frame the automatic grading task, it deliberately employed specific prompt engineering heuristics (or “tricks,” as I’ll call them). These (and other) tricks have emerged as reliable ways to improve the quality of LLM responses."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "7 Tricks for Prompt Engineering",
        "text": "Although there are many tips and tricks for writing good prompts, here I restrict the discussion to the ones that seem the most fundamental (IMO) based on a handful of references [1,3–5]. For a deeper dive, I recommend the reader explore the sources cited here."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "7 Tricks for Prompt Engineering",
        "text": "A defining feature of LLMs is that they are trained on massive text corpora. This equips them with a vast knowledge of the world and the ability to perform an enormous variety of tasks. However, this impressive generality may hinder performance on a specific task if the proper context is not provided."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "7 Tricks for Prompt Engineering",
        "text": "For example, let’s compare two prompts for generating a birthday message for my dad."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "7 Tricks for Prompt Engineering",
        "text": "The next trick is to give the LLM example responses to improve its performance on a particular task. The technical term for this is few-shot learning, and has been shown to improve LLM performance significantly [6]."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "7 Tricks for Prompt Engineering",
        "text": "Let’s look at a specific example. Say we want to write a subtitle for a Towards Data Science article. We can use existing examples to help guide the LLM completion."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "7 Tricks for Prompt Engineering",
        "text": "Ensuring prompts follow an organized structure not only makes them easier to read and write, but also tends to help the model generate good completions. We employed this technique in the example for Trick 2, where we explicitly labeled the title and subtitle for each example."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "7 Tricks for Prompt Engineering",
        "text": "However, there are countless ways we can give our prompts structure. Here are a handful of examples: use ALL CAPS for emphasis, use delimiters like ``` to highlight a body of text, use markup languages like Markdown or HTML to format text, use JSON to organize information, etc.  Now, let’s see this in action."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "7 Tricks for Prompt Engineering",
        "text": "This trick was proposed by Wei et al. [7]. The basic idea is to guide an LLM to think “step by step”. This helps break down complex problems into manageable sub-problems, which gives the LLM “time to think” [3,5]. Zhang et al. showed that this could be as simple as including the text “Let’s think step by step” in the prompt [8]."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "7 Tricks for Prompt Engineering",
        "text": "This notion can be extended to any recipe-like process. For example, if I want to create a LinkedIn post based on my latest Medium blog, I can guide the LLM to mirror the step-by-step process I follow."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "7 Tricks for Prompt Engineering",
        "text": "A somewhat surprising technique that tends to improve LLM performance is to prompt it to take on a particular persona e.g. “you are an expert”. This is helpful because you may not know the best way to describe your problem to the LLM, but you may know who would help you solve that problem [1]. Here’s what this might look like in practice."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "7 Tricks for Prompt Engineering",
        "text": "It can be difficult to optimally prompt an LLM when we do not know what it knows or how it thinks. That is where the “flipped approach” can be helpful. This is where you prompt the LLM to ask you questions until it has a sufficient understanding (i.e. context) of the problem you are trying to solve."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "7 Tricks for Prompt Engineering",
        "text": "This final trick prompts the model to reflect on its past responses to improve them. Common use cases are having the model critically evaluate its own work by asking it if it “completed the assignment” or having it “explain the reasoning and assumptions” behind a response [1, 3]."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "7 Tricks for Prompt Engineering",
        "text": "Additionally, you can ask the LLM to refine not only its responses but your prompts. This is a simple way to automatically rewrite prompts so that they are easier for the model to “understand”."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Example Code: Automatic Grader with LangChain",
        "text": "Now that we’ve reviewed several prompting heuristics let’s see how we can apply them to a specific use case. To do this, we will return to the automatic grader example from before."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Example Code: Automatic Grader with LangChain",
        "text": "On second look, a few of the previously mentioned tricks should be apparent i.e. Trick 6: chatbot persona, Trick 3: use structured text, and Trick 1: be descriptive. This is what good prompting typically looks like in practice, namely combining multiple techniques in a single prompt."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Example Code: Automatic Grader with LangChain",
        "text": "While we could copy-paste this prompt template into ChatGPT and replace the question, correct_answer, and student_answer fields, this is not a scalable way to implement the automatic grader. Rather, what we want is to integrate this prompt into a larger software system so that we can build a user-friendly application that a human can use."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Example Code: Automatic Grader with LangChain",
        "text": "One way we can do this is via LangChain, which is a Python library that helps simplify building applications on top of large language models. It does this by providing a variety of handy abstractions for using LLMs programmatically."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Example Code: Automatic Grader with LangChain",
        "text": "The central class that does this is called chain (hence the library name). This abstracts the process of generating a prompt, sending it to an LLM, and parsing the output so that it can be easily called and integrated into a larger script."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Example Code: Automatic Grader with LangChain",
        "text": "Let’s see how to use LangChain for our automatic grader use case. The example code is available on the GitHub Repo for this article."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Example Code: Automatic Grader with LangChain",
        "text": "We first start by importing the necessary library modules."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Example Code: Automatic Grader with LangChain",
        "text": "Here we will use gpt-3.5-turbo which requires a secret key for OpenAI’s API. If you don’t have one, I gave a step-by-step guide on how to get one in a past article of this series. I like to store the secret key in a separate Python file (sk.py) and import it with the following line of code."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Example Code: Automatic Grader with LangChain",
        "text": "To define our chain, we need two core elements: the LLM and the prompt. We start by creating an object for the LLM."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Example Code: Automatic Grader with LangChain",
        "text": "LangChain has a class specifically for OpenAI (and many other) chat models. I pass in my secret API key and set the temperature to 0. The default model here is gpt-3.5-turbo, but you can alternatively use gpt-4 using the “model_name” input argument. You can further customize the chat model by setting other input arguments."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Example Code: Automatic Grader with LangChain",
        "text": "Next, we define our prompt template. This object allows us to generate prompts dynamically via input strings that automatically update a base template. Here’s what that looks like."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Example Code: Automatic Grader with LangChain",
        "text": "With our LLM and prompt, we can now define our chain."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Example Code: Automatic Grader with LangChain",
        "text": "Next, we can pass inputs to the chain and obtain a grade in one line of code."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Example Code: Automatic Grader with LangChain",
        "text": "While this chain can perform the grading task effectively, its outputs may not be suitable for an automated process. For instance, in the above code block, the LLM correctly said the student’s answer of “FDR” was wrong, but it would be better if the LLM gave us an output in a standard format that could be used in downstream processing."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Example Code: Automatic Grader with LangChain",
        "text": "This is where output parsers come in handy. These are functions we can integrate into a chain to convert LLM outputs to a standard format. Let’s see how we can make an output parser that converts the LLM response to a boolean (i.e. True or False) output."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Example Code: Automatic Grader with LangChain",
        "text": "Here, we create a simple output parser that checks if the word “wrong” is in the LLM’s output. If not, we return True, indicating the student's correct answer. Otherwise, we return False, indicating the student's answer was incorrect."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Example Code: Automatic Grader with LangChain",
        "text": "We can then incorporate this output parser into our chain to seamlessly parse text when we run the chain."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Example Code: Automatic Grader with LangChain",
        "text": "Finally, we can run the chain for a whole list of student answers and print the outputs."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Limitations",
        "text": "Prompt Engineering is more than asking ChatGPT for help writing an email or learning about Quantum Computing. It is a new programming paradigm that changes how developers can build applications."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Limitations",
        "text": "While this is a powerful innovation, it has its limitations. For one, optimal prompting strategies are LLM-dependent. For example, prompting GPT-3 to “think step-by-step” resulted in significant performance gains on simple mathematical reasoning tasks [8]. However, for the latest version of ChatGPT, the same strategy doesn’t seem helpful (it already thinks step-by-step)."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Limitations",
        "text": "Another limitation of Prompt Engineering is it requires large-scale general-purpose language models such as ChatGPT, which come at significant computational and financial costs. This may be overkill for many use cases that are more narrowly defined e.g. string matching, sentiment analysis, or text summarization."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Limitations",
        "text": "We can overcome both these limitations via fine-tuning pre-trained language models. This is where we take an existing language model and tweak it for a particular use case. In the next article of this series, we will explore popular fine-tuning techniques supplemented with example Python code."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Limitations",
        "text": "👉 More on LLMs: Introduction | OpenAI API | Hugging Face Transformers | Fine-tuning | Build an LLM | QLoRA | RAG | Text Embeddings"
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Resources",
        "text": "Connect: My website | Book a call | Ask me anything"
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Resources",
        "text": "Socials: YouTube 🎥 | LinkedIn | Twitter"
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Resources",
        "text": "[3] State of GPT by Andrej Karpathy at Microsoft Build 2023"
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Resources",
        "text": "[5] ChatGPT Prompt Engineering for Developers by deeplearning.ai"
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Resources",
        "text": "By Shaw Talebi on August 25, 2023."
    },
    {
        "article_title": "Prompt Engineering — How to trick AI into solving your problems",
        "section": "Resources",
        "text": "Exported from Medium on December 2, 2024."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Text Embeddings, Classification, and Semantic Search",
        "text": "This article is part of a larger series on using large language models (LLMs) in practice. In the previous post, we saw how to improve an LLM via retrieval-augmented generation (i.e. RAG). A key part of RAG was using text embeddings to retrieve relevant information from a knowledge base automatically. Here, I will discuss text embeddings more deeply and share two simple (yet powerful) applications: text classification and semantic search."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Text Embeddings, Classification, and Semantic Search",
        "text": "ChatGPT captured the world’s imagination regarding AI and its potential. A key contributor to this impact was ChatGPT’s chat interface, which made the power of AI more accessible than ever before."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Text Embeddings, Classification, and Semantic Search",
        "text": "While this unlocked a new level of AI hype and awareness, all the excitement around this “chatbot paradigm” left another key innovation (largely) unnoticed."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Text Embeddings, Classification, and Semantic Search",
        "text": "LLMs brought major innovations in text embeddings. Here, I’ll explain these and how we can use them for simple yet high-value use cases."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Text embeddings",
        "text": "Text embeddings translate words into numbers. However, these aren’t just any numbers. They are numbers that capture the meaning of the underlying text."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Text embeddings",
        "text": "This is important because numbers are (much) easier to analyze than words."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Text embeddings",
        "text": "For example, if you are at a networking event and want to know the typical height of people in the room, you can measure everyone’s height and compute the average using Microsoft Excel. However, if you want to know the typical job title of the people in the room, there’s no Excel function to help you."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Text embeddings",
        "text": "This is where text embeddings can help. If we have a good way to turn text into numbers, we unlock a massive toolbox of existing statistical and machine-learning techniques to investigate textual data."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Text embeddings (Visually Explained)",
        "text": "Let's look at a visual example to better understand what it means to “translate text into numbers” [1]."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Text embeddings (Visually Explained)",
        "text": "Consider the following set of text: tree, lotus flower, daisy, sun, Saturn, Jupiter, satellite, space shuttle, basketball, and baseball."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Text embeddings (Visually Explained)",
        "text": "While this may seem like a random assortment of words, some of these concepts are more similar than others. We can convey these similarities (and differences) in the following way."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Text embeddings (Visually Explained)",
        "text": "The above visualization intuitively organizes the concepts. Similar items (e.g., tree, daisy, and lotus flower) tend to be close together, while dissimilar items (e.g., tree and baseball) tend to be far apart."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Text embeddings (Visually Explained)",
        "text": "Numbers fit into this picture because we can assign coordinates to each word based on its location in the plot above. These coordinates (i.e. numbers) can then be used to analyze the underlying text."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Where do they come from?",
        "text": "Turning text into numbers to make it more computable is not a new idea. Researchers have explored this since the early days of computing (circa 1950) [2]."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Where do they come from?",
        "text": "While there are countless ways people have done this over the years [2], these days, state-of-the-art text representations are derived from large language models (LLMs)."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Where do they come from?",
        "text": "This works because LLMs learn (very) good numerical representations of text during their training. The layers that generate these representations can be dissected from the model and used in a stand-alone way. The result of this process is a text embedding model."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Can’t I just use ChatGPT?",
        "text": "Before moving on to the example use cases, you might think, “Shaw, why should I care about these text embedding things? Can’t I just make a custom ChatGPT to analyze the text for me?”"
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Can’t I just use ChatGPT?",
        "text": "Of course, one can use techniques like RAG or Fine-tuning to build an AI agent tailored to their specific problem set. However, it’s still the early days for these systems, which makes building a robust AI agent (i.e. not a prototype) an expensive and non-trivial engineering problem (e.g. major computational costs, LLM security risks, unpredictable responses, and hallucinations)"
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Can’t I just use ChatGPT?",
        "text": "On the other hand, text embeddings have been around for decades, are lightweight, and non-stochastic (i.e. predictable). Thus, building AI systems with them is much simpler and cheaper than building an AI agent (while still capturing much of the value — if not more)."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 1: Text Classification",
        "text": "With a basic understanding of text embeddings, let’s see how we can use them to help solve real-world problems."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 1: Text Classification",
        "text": "First, we will discuss text classification. This consists of assigning a label to a given text. For example, labeling an email as spam or not spam, a credit application as high risk or low risk, or a security alert as the real deal or a false alarm."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 1: Text Classification",
        "text": "In this example, I will use text embeddings to classify resumes as “Data Scientist” or “Not Data Scientist,” which may be relevant for recruiters trying to navigate an ocean of job candidates."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 1: Text Classification",
        "text": "To avoid privacy issues, I created a synthetic dataset of resumes using gpt-3.5-turbo. While using synthetic data requires us to take the results with a grain of salt, this example still provides an instructive demonstration of how to use text embeddings to classify."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 1: Text Classification",
        "text": "The example code and data are freely available at the GitHub repository."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 1: Text Classification",
        "text": "We start by importing dependencies. In this example, I use a text embedding model from OpenAI, which requires an API key. If you are unfamiliar with the OpenAI API, I give a beginner-friendly primer in a previous article of this series. Here, my API key is stored in a separate file called sk.py."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 1: Text Classification",
        "text": "Next, we read our synthetic training dataset as a Pandas dataframe. The data comes from a .csv file with two columns consisting of resume text and the associated role."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 1: Text Classification",
        "text": "To translate the resumes into embeddings, we can make a simple API call to the OpenAI API. This is done by the function below."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 1: Text Classification",
        "text": "We can now apply this function to each resume in our dataframe and store the result in a list."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 1: Text Classification",
        "text": "Next, we’ll create a new dataframe to store the text embeddings and our target variable for model training."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 1: Text Classification",
        "text": "With our training data prepared we can now train our classification model in one line of code. Here, I use a Random Forest classifier, which I discussed in a past article."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 1: Text Classification",
        "text": "To get a quick sense of the model’s performance we can evaluate it on the training data. Here, I compute the mean accuracy and area under the ROC (i.e., AUC)."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 1: Text Classification",
        "text": "The accuracy and AUC values of 1 indicate perfect performance on the training dataset, which is suspicious. So let’s evaluate it on a testing dataset the model has never seen before."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 1: Text Classification",
        "text": "Although the model performs well when applied to the testing data, it is still likely overfitting for two reasons."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 1: Text Classification",
        "text": "One, we have 1537 predictors and only 100 resumes to predict, so it wouldn’t be hard for the model to “memorize” every example in the training data. Two, the training and testing data were generated from gpt-3.5-turbo in a similar way. Thus, they share many characteristics, which makes the classification task easier than if applied to real data."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 1: Text Classification",
        "text": "There are many tricks we can employ to overcome the overfitting problem, e.g., reducing predictor count using predictor importance ranking, increasing the minimum number of samples in a leaf node, or using a simpler classification technique like logistic regression. However, if our goal is to use this model in a practical setting, the best option would be to gather more data and use resumes from the real world."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 2: Semantic Search",
        "text": "Next, let’s look at semantic search. In contrast to keyword-based search, semantic search generates results based on the meaning of a user’s query rather than the particular words or phrases used."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 2: Semantic Search",
        "text": "For example, keyword-based searches may not provide great results for the query “I need someone to build out my data infrastructure” since it doesn’t specifically mention the role that builds data infrastructure (i.e., a data engineer). However, this is not a concern for semantic search since it can match the query to candidates with experience like “Proficient in data modeling, ETL processes, and data warehousing.”"
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 2: Semantic Search",
        "text": "Here, we will use text embeddings to enable this type of search over the same dataset as in the previous use case. Example code is (again) available at the GitHub repo."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 2: Semantic Search",
        "text": "We start by importing dependencies and the synthetic dataset."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 2: Semantic Search",
        "text": "Next, we’ll generate the text embeddings. Instead of using the OpenAI API, we will use an open-source model from the Sentence Transformers Python library. This model was specifically fine-tuned for semantic search."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 2: Semantic Search",
        "text": "To see the different resumes in the dataset and their relative locations in concept space, we can use PCA to reduce the dimensionality of the embedding vectors and visualize the data on a 2D plot (code is on GitHub)."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 2: Semantic Search",
        "text": "From this view we see the resumes for a given role tend to clump together."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 2: Semantic Search",
        "text": "Now, to do a semantic search over these resumes, we can take a user query, translate it into a text embedding, and then return the nearest resumes in the embedding space. Here’s what that looks like in code."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 2: Semantic Search",
        "text": "Printing the roles of the top 10 results, we see almost all are data engineers, which is a good sign."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 2: Semantic Search",
        "text": "Let’s look at the resume of the top search results."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 2: Semantic Search",
        "text": "Although this is a made-up resume, the candidate likely has all the necessary skills and experience to fulfill the user’s needs."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 2: Semantic Search",
        "text": "Another way to look at the search results is via the 2D plot from before. Here’s what that looks like for a few queries (see plot titles)."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 2: Semantic Search",
        "text": "While this simple search example does a good job of matching particular candidates to a given query, it is not perfect. One shortcoming is when the user query includes a specific skill. For example, in the query “Data Engineer with Apache Airflow experience,” only 1 of the top 5 results have Airflow experience."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 2: Semantic Search",
        "text": "This highlights that semantic search is not better than keyword-based search in all situations. Each has its strengths and weaknesses."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 2: Semantic Search",
        "text": "Thus, a robust search system will employ so-called hybrid search, which combines the best of both techniques. While there are many ways to design such a system, a simple approach is applying keyword-based search to filter down results, followed by semantic search."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 2: Semantic Search",
        "text": "Two additional strategies for improving search are using a Reranker and fine-tuning text embeddings."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 2: Semantic Search",
        "text": "A Reranker is a model that directly compares two pieces of text. In other words, instead of computing the similarity between pieces of text via a distance metric in the embedding space, a Reranker computes such a similarity score directly."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 2: Semantic Search",
        "text": "Rerankers are commonly used to refine search results. For example, one can return the top 25 results using semantic search and then refine to the top 5 with a Reranker."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Use Case 2: Semantic Search",
        "text": "Fine-tuning text embeddings involves adapting an embedding model for a particular domain. This is a powerful approach because most embedding models are based on a broad collection of text and knowledge. Thus, they may not optimally organize concepts for a specific industry, e.g. data science and AI."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Conclusion",
        "text": "Although everyone seems focused on the potential for AI agents and assistants, recent innovations in text-embedding models have unlocked countless opportunities for simple yet high-value ML use cases."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Conclusion",
        "text": "Here, we reviewed two widely applicable use cases: text classification and semantic search. Text embeddings enable simpler and cheaper alternatives to LLM-based methods while still capturing much of the value."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Resources",
        "text": "Connect: My website | Book a call"
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Resources",
        "text": "Socials: YouTube 🎥 | LinkedIn | Instagram"
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Resources",
        "text": "[1] https://youtu.be/A8HEPBdKVMA?si=PA4kCnfgd3nx24LR"
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Resources",
        "text": "[2] R. Patil, S. Boit, V. Gudivada and J. Nandigam, “A Survey of Text Representation and Embedding Techniques in NLP,” in IEEE Access, vol. 11, pp. 36120–36146, 2023, doi: 10.1109/ACCESS.2023.3266377."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Resources",
        "text": "By Shaw Talebi on March 27, 2024."
    },
    {
        "article_title": "Text Embeddings, Classification, and Semantic Search",
        "section": "Resources",
        "text": "Exported from Medium on December 2, 2024."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "How to Improve LLMs with RAG",
        "text": "This article is part of a larger series on using large language models in practice. In the previous post, we fine-tuned Mistral-7b-Instruct to respond to YouTube comments using QLoRA. Although the fine-tuned model successfully captured my style when responding to viewer feedback, its responses to technical questions didn’t match my explanations. Here, I’ll discuss how we can improve LLM performance using retrieval augmented generation (i.e. RAG)."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "How to Improve LLMs with RAG",
        "text": "Large language models (LLMs) have demonstrated an impressive ability to store and deploy vast knowledge in response to user queries. While this has enabled the creation of powerful AI systems like ChatGPT, compressing world knowledge in this way has two key limitations."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "How to Improve LLMs with RAG",
        "text": "First, an LLM’s knowledge is static, i.e., not updated as new information becomes available. Second, LLMs may have an insufficient “understanding” of niche and specialized information that was not prominent in their training data. These limitations can result in undesirable (and even fictional) model responses to user queries."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "How to Improve LLMs with RAG",
        "text": "One way we can mitigate these limitations is to augment a model via a specialized and mutable knowledge base, e.g., customer FAQs, software documentation, or product catalogs. This enables the creation of more robust and adaptable AI systems."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "How to Improve LLMs with RAG",
        "text": "Retrieval augmented generation, or RAG, is one such approach. Here, I provide a high-level introduction to RAG and share example Python code for implementing a RAG system using LlamaIndex."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "What is RAG?",
        "text": "The basic usage of an LLM consists of giving it a prompt and getting back a response."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "What is RAG?",
        "text": "RAG works by adding a step to this basic process. Namely, a retrieval step is performed where, based on the user’s prompt, the relevant information is extracted from an external knowledge base and injected into the prompt before being passed to the LLM."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Why we care",
        "text": "Notice that RAG does not fundamentally change how we use an LLM; it's still prompt-in and response-out. RAG simply augments this process (hence the name)."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Why we care",
        "text": "This makes RAG a flexible and (relatively) straightforward way to improve LLM-based systems. Additionally, since knowledge is stored in an external database, updating system knowledge is as simple as adding or removing records from a table."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Why we care",
        "text": "Previous articles in this series discussed fine-tuning, which adapts an existing model for a particular use case. While this is an alternative way to endow an LLM with specialized knowledge, empirically, fine-tuning seems to be less effective than RAG at doing this [1]."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "How it works",
        "text": "There are 2 key elements of a RAG system: a retriever and a knowledge base."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "How it works",
        "text": "A retriever takes a user prompt and returns relevant items from a knowledge base. This typically works using so-called text embeddings, numerical representations of text in concept space. In other words, these are numbers that represent the meaning of a given text."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "How it works",
        "text": "Text embeddings can be used to compute a similarity score between the user’s query and each item in the knowledge base. The result of this process is a ranking of each item’s relevance to the input query."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "How it works",
        "text": "The retriever can then take the top k (say k=3) most relevant items and inject them into the user prompt. This augmented prompt is then passed into the LLM for generation."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "How it works",
        "text": "The next key element of a RAG system is a knowledge base. This houses all the information you want to make available to the LLM. While there are countless ways to construct a knowledge base for RAG, here I’ll focus on building one from a set of documents."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "How it works",
        "text": "The process can be broken down into 4 key steps [2,3]."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "How it works",
        "text": "Load docs — This consists of gathering a collection of documents and ensuring they are in a ready-to-parse format (more on this later).Chunk docs—Since LLMs have limited context windows, documents must be split into smaller chunks (e.g., 256 or 512 characters long).Embed chunks — Translate each chunk into numbers using a text embedding model.Load into Vector DB— Load text embeddings into a database (aka a vector database)."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Some Nuances",
        "text": "While the steps for building a RAG system are conceptually simple, several nuances can make building one (in the real world) more complicated."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Some Nuances",
        "text": "Document preparation—The quality of a RAG system is driven by how well useful information can be extracted from source documents. For example, if a document is unformatted and full of images and tables, it will be more difficult to parse than a well-formatted text file."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Some Nuances",
        "text": "Choosing the right chunk size—We already mentioned the need for chunking due to LLM context windows. However, there are 2 additional motivations for chunking."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Some Nuances",
        "text": "First, it keeps (compute) costs down. The more text you inject into the prompt, the more compute required to generate a completion. The second is performance. Relevant information for a particular query tends to be localized in source documents (often, just 1 sentence can answer a question). Chunking helps minimize the amount of irrelevant information passed into the model [4]."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Some Nuances",
        "text": "Improving search — While text embeddings enable a powerful and fast way to do search, it doesn’t always work as one might hope. In other words, it may return results that are “similar” to the user query, yet not helpful for answering it, e.g., “How’s the weather in LA?” may return “How’s the weather in NYC?”."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Some Nuances",
        "text": "The simplest way to mitigate this is through good document preparation and chunking. However, for some use cases, additional strategies for improving search might be necessary, such as using meta-tags for each chunk, employing hybrid search, which combines keyword—and embedding-based search, or using a reranker, which is a specialized model that computes the similarity of 2 input pieces of text."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "With a basic understanding of how RAG works, let’s see how to use it in practice. I will build upon the example from the previous article, where I fine-tuned Mistral-7B-Instruct to respond to YouTube comments using QLoRA. We will use LlamaIndex to add a RAG system to the fine-tuned model from before."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "The example code is freely available in a Colab Notebook, which can run on the (free) T4 GPU provided. The source files for this example are available at the GitHub repository."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "We start by installing and importing necessary Python libraries."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "We can configure our knowledge base by defining our embedding model, chunk size, and chunk overlap. Here, we use the ~33M parameter bge-small-en-v1.5 embedding model from BAAI, which is available on the Hugging Face hub. Other embedding model options are available on this text embedding leaderboard."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "Next, we load our source documents. Here, I have a folder called “articles,” which contains PDF versions of 3 Medium articles I wrote on fat tails. If running this in Colab, you must download the articles folder from the GitHub repo and manually upload it to your Colab environment."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "For each file in this folder, the function below will read the text from the PDF, split it into chunks (based on the settings defined earlier), and store each chunk in a list called documents."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "Since the blogs were downloaded directly as PDFs from Medium, they resemble a webpage more than a well-formatted article. Therefore, some chunks may include text unrelated to the article, e.g., webpage headers and Medium article recommendations."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "In the code block below, I refine the chunks in documents, removing most of the chunks before or after the meat of an article."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "Finally, we can store the refined chunks in a vector database."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "With our knowledge base in place, we can create a retriever using LlamaIndex’s VectorIndexRetreiver(), which returns the top 3 most similar chunks to a user query."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "Next, we define a query engine that uses the retriever and query to return a set of relevant chunks."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "Now, with our knowledge base and retrieval system set up, let’s use it to return chunks relevant to a query. Here, we’ll pass the same technical question we asked ShawGPT (the YouTube comment responder) from the previous article."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "The query engine returns a response object containing the text, metadata, and indexes of relevant chunks. The code block below returns a more readable version of this information."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "We start by downloading the fine-tuned model from the Hugging Face hub."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "As a baseline, we can see how the model responds to the technical question without any context from the articles. To do this, we create a prompt template using a lambda function, which takes in a viewer comment and returns a prompt for the LLM. For more details on where this prompt comes from, see the previous article of this series."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "Next, we can pass this prompt to the model using the code below."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "Here’s the model’s response (no context)."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "Although the response's style and formatting are great, the model's explanation of fat-tailedness differs from how I defined it in my video and blog series."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "Let’s see what happens to the model’s response when we include the appropriate context. To do this, we create another prompt template, which can also take in context from the RAG system."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "Next, we pass the prompt with context from the query engine and the view comment to the model."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "Here’s the new response (with context)."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Example code: Improving YouTube Comment Responder with RAG",
        "text": "This does a much better job of capturing my explanation of fat tails than the no-context response and even calls out the niche concepts of Mediocristan and Extremistan."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "What’s next?",
        "text": "Here, I gave a beginner-friendly introduction to RAG and shared a concrete example of how to implement it using LlamaIndex. RAG allows us to improve an LLM system with updateable and domain-specific knowledge."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "What’s next?",
        "text": "While much of the recent AI hype has centered around building AI assistants, a powerful (yet less popular) innovation has come from text embeddings (i.e. the things we used to do retrieval). In the next article of this series, I will explore text embeddings in more detail, including how they can be used for semantic search and classification tasks."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Resources",
        "text": "Connect: My website | Book a call"
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Resources",
        "text": "Socials: YouTube 🎥 | LinkedIn | Instagram"
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Resources",
        "text": "[2] LlamaIndex Webinar: Building LLM Apps for Production, Part 1 (co-hosted with Anyscale)"
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Resources",
        "text": "[4] LlamaIndex Webinar: Make RAG Production-Ready"
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Resources",
        "text": "By Shaw Talebi on March 9, 2024."
    },
    {
        "article_title": "How to Improve LLMs with RAG",
        "section": "Resources",
        "text": "Exported from Medium on December 2, 2024."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Multimodal Models — LLMs That Can See and Hear",
        "text": "This is the first post in a larger series on Multimodal AI. A Multimodal Model (MM) is an AI system capable of processing or generating multiple data modalities (e.g., text, image, audio, video). In this article, I will discuss a particular type of MM that builds on top of a large language model (LLM). I’ll start with a high-level overview of such models and then share example code for using LLaMA 3.2 Vision to perform various image-to-text tasks."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Multimodal Models — LLMs That Can See and Hear",
        "text": "Large language models (LLMs) have marked a fundamental shift in AI research and development. However, despite their broader impacts, they are still fundamentally limited."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Multimodal Models — LLMs That Can See and Hear",
        "text": "Namely, LLMs can only process and generate text, making them blind to other modalities such as images, video, audio, and more. This is a major limitation since some tasks rely on non-text data, e.g., analyzing engineering blueprints, reading body language or speech tonality, and interpreting plots and infographics."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Multimodal Models — LLMs That Can See and Hear",
        "text": "This has sparked efforts toward expanding LLM functionality to include multiple modalities."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "What is a Multimodal Model?",
        "text": "A Multimodal Model (MM) is an AI system that can process multiple data modalities as input or output (or both) [1]. Below are a few examples."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "What is a Multimodal Model?",
        "text": "GPT-4o — Input: text, images, and audio. Output: text.FLUX — Input: text. Output: images.Suno — Input: text. Output: audio."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "What is a Multimodal Model?",
        "text": "While there are several ways to create models that can process multiple data modalities, a recent line of research seeks to use LLMs as the core reasoning engine of a multimodal system [2]. Such models are called multimodal large language models (or large multimodal models) [2][3]."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "What is a Multimodal Model?",
        "text": "One benefit of using existing LLM as a starting point for MMs is that they’ve demonstrated a strong ability to acquire world knowledge through large-scale pre-training, which can be leveraged to process concepts appearing in non-textual representations."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "3 Paths to Multimodality",
        "text": "Here, I will focus on multimodal models developed from an LLM. Three popular approaches are described below."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "3 Paths to Multimodality",
        "text": "LLM + Tools: Augment LLMs with pre-built componentsLLM + Adapters: Augment LLMs with multi-modal encoders or decoders, which are aligned via adapter fine-tuningUnified Models: Expand LLM architecture to fuse modalities at pre-training"
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Path 1: LLM + Tools",
        "text": "The simplest way to make an LLM multimodal is by adding external modules that can readily translate between text and an arbitrary modality. For example, a transcription model (e.g. Whisper) can be connected to an LLM to translate input speech into text, or a text-to-image model can generate images based on LLM outputs."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Path 1: LLM + Tools",
        "text": "The key benefit of such an approach is simplicity. Tools can quickly be assembled without any additional model training."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Path 1: LLM + Tools",
        "text": "The downside, however, is that the quality of such a system may be limited. Just like when playing a game of telephone, messages mutate when passed from person to person. Information may degrade going from one module to another via text descriptions only."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Path 2: LLM + Adapters",
        "text": "One way to mitigate the “telephone problem” is by optimizing the representations of new modalities to align with the LLM’s internal concept space. For example, ensuring an image of a dog and the description of one look similar to the LLM."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Path 2: LLM + Adapters",
        "text": "This is possible through the use of adapters, a relatively small set of parameters that appropriately translate a dense vector representation for a downstream model [2][4][5]."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Path 2: LLM + Adapters",
        "text": "Adapters can be trained using, for example, image-caption pairs, where the adapter learns to translate an image encoding into a representation compatible with the LLM [2][4][6]. One way to achieve this is via contrastive learning [2], which I will discuss more in the next article of this series."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Path 2: LLM + Adapters",
        "text": "The benefits of using adapters to augment LLMs include better alignment between novel modality representations in a data-efficient way. Since many pre-trained embedding, language, and diffusion models are available in today’s AI landscape, one can readily fuse models based on their needs. Notable examples from the open-source community are LLaVA, LLaMA 3.2 Vision, Flamingo, MiniGPT4, Janus, Mini-Omni2, and IDEFICS [3][5][7][8]."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Path 2: LLM + Adapters",
        "text": "However, this data efficiency comes at a price. Just like how adapter-based fine-tuning approaches (e.g. LoRA) can only nudge an LLM so far, the same holds in this context. Additionally, pasting various encoders and decoders to an LLM may result in overly complicated model architectures."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Path 3: Unified Models",
        "text": "The final way to make an LLM multimodal is by incorporating multiple modalities at the pre-training stage. This works by adding modality-specific tokenizers (rather than pre-trained encoder/decoder models) to the model architecture and expanding the embedding layer to accommodate new modalities [9]."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Path 3: Unified Models",
        "text": "While this approach comes with significantly greater technical challenges and computational requirements, it enables the seamless integration of multiple modalities into a shared concept space, unlocking better reasoning capabilities and efficiencies [10]."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Path 3: Unified Models",
        "text": "The preeminent example of this unified approach is (presumably) GPT-4o, which processes text, image, and audio inputs to enable expanded reasoning capabilities at faster inference times than previous versions of GPT-4. Other models that follow this approach include Gemini, Emu3, BLIP, and Chameleon [9][10]."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Path 3: Unified Models",
        "text": "Training these models typically entails multi-step pre-training on a set of (multimodal) tasks, such as language modeling, text-image contrastive learning, text-to-video generation, and others [7][9][10]."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "With a basic understanding of how LLM-based multimodal models work under the hood, let’s see what we can do with them. Here, I will use LLaMA 3.2 Vision to perform various image-to-text tasks."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "To run this example, download Ollama and its Python library. This enables the model to run locally i.e. no need for external API calls."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "The example code is freely available on GitHub."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "Next, we’ll download the model locally. Here, we use LLaMA 3.2 Vision 11B."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "Now, we’re ready to use the model! Here’s how we can do basic visual question answering."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "The image is of me from a networking event (as shown below)."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "The model’s response is shown below. While it has trouble reading what’s on my hat, it does a decent job inferring the context of the photo."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "If you run this on your machine, you may run into a long wait time until the model generates a response. One thing we can do to make this less painful is to enable streaming."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "Interestingly, we get a qualitatively different response when prompting the model in a slightly different way for the same image."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "Objectively describing a scene is simpler than understanding and explaining humor. Let’s see how the model explains the meme below."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "The model does a good job here. It understands that the image is funny while also conveying the pain that people face."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "The last use case is optical character recognition (OCR). This involves extracting text from images, which is valuable in a wide range of contexts. Here, I’ll see if the model can translate a screenshot from my notes app to a markdown file."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "Example: Using LLaMA 3.2 Vision for Image-based Tasks",
        "text": "Again, the model does a decent job out of the box. While it missed the header, it accurately captured the content and formatting of the project ideas."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "What’s next?",
        "text": "Multimodal models are AI systems that can process multiple data modalities as inputs or outputs (or both). A recent trend for developing these systems involves adding modalities to large language models (LLMs)."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "What’s next?",
        "text": "However, there are other types of multimodal models. In the next article of this series, I will discuss multimodal embedding models, which encode multiple data modalities (e.g. text and images) into a shared representation space."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "What’s next?",
        "text": "👉 Get FREE access to every new story I write (Learn More)"
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "What’s next?",
        "text": "[1] Multimodal Machine Learning: A Survey and Taxonomy"
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "What’s next?",
        "text": "[2] A Survey on Multimodal Large Language Models"
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "What’s next?",
        "text": "[5] Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation"
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "What’s next?",
        "text": "[6] Learning Transferable Visual Models From Natural Language Supervision"
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "What’s next?",
        "text": "[7] Flamingo: a Visual Language Model for Few-Shot Learning"
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "What’s next?",
        "text": "[8] Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities"
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "What’s next?",
        "text": "[9] Emu3: Next-Token Prediction is All You Need"
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "What’s next?",
        "text": "[10] Chameleon: Mixed-Modal Early-Fusion Foundation Models"
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "What’s next?",
        "text": "By Shaw Talebi on November 19, 2024."
    },
    {
        "article_title": "Multimodal Models — LLMs that can see and hear",
        "section": "What’s next?",
        "text": "Exported from Medium on December 2, 2024."
    }
]