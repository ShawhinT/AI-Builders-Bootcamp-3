{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22c6a7a2-26c9-4e14-aafa-5cf069d02e46",
   "metadata": {},
   "source": [
    "# Article Series QA Assistant with RAG\n",
    "## ABB #3 - Session 4\n",
    "\n",
    "Code authored by: Shaw Talebi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdff7bfa-ebdb-4323-b16a-d69aa948eb5b",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3ebb26-94ac-4c47-a5fc-170c8caa0cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "from functions import *\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "953a58cc-0b6b-4529-a2af-c1f58facd0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sk from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# setup api client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe890f-5c07-44c9-8afb-dbc819e82660",
   "metadata": {},
   "source": [
    "### load data & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029c926b-6f42-4af6-816a-e34dc20aab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load chunks\n",
    "filename = 'data/chunk_list.json'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    chunk_list = json.load(f)\n",
    "\n",
    "# load embeddings\n",
    "chunk_embeddings = torch.load('data/chunk_embeddings.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07c9b8c5-105a-492e-a14e-fb711e735f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num chunks: 778\n",
      "(778, 384)\n"
     ]
    }
   ],
   "source": [
    "print(\"Num chunks:\",len(chunk_list))\n",
    "print(chunk_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fff575e-faed-4d42-9c19-c4325f1e8e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109f511-8897-4e69-aa99-30a2ff79b0c9",
   "metadata": {},
   "source": [
    "### 1) define query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da59e64-ba7b-4150-8615-e23fe9b71a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define query\n",
    "query = \"When does it make sense to use RAG vs fine-tuning?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba55143-e771-4603-98a6-80b9f758d26d",
   "metadata": {},
   "source": [
    "### 2) context retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce377b19-c0ec-4519-9cd1-e119911f109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_markdown = semantic_search(query, model, chunk_embeddings, chunk_list, temp=0.1, k=10, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d16694e7-6dc2-4630-80cb-e1d087e38f7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** We’ve already mentioned situations where RAG and fine-tuning perform well. However, since this is such a common question, it’s worth reemphasizing when each approach works best.  \n",
       "\n",
       "2. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** Here’s high-level guidance on when to use each.  \n",
       "\n",
       "3. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** Why we care  \n",
       "   **Snippet:** Previous articles in this series discussed fine-tuning, which adapts an existing model for a particular use case. While this is an alternative way to endow an LLM with specialized knowledge, empirically, fine-tuning seems to be less effective than RAG at doing this [1].  \n",
       "\n",
       "4. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** RAG is when we inject relevant context into an LLM’s input prompt so that it can generate more helpful responses. For example, if we have a domain-specific knowledge base (e.g., internal company documents and emails), we might identify the items most relevant to the user’s query so that an LLM can synthesize information in an accurate and digestible way.  \n",
       "\n",
       "5. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** Notice that these approaches are not mutually exclusive. In fact, the original RAG system proposed by Facebook researchers used fine-tuning to better use retrieved information for generating responses [4].  \n",
       "\n",
       "6. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** Some Nuances  \n",
       "   **Snippet:** Document preparation—The quality of a RAG system is driven by how well useful information can be extracted from source documents. For example, if a document is unformatted and full of images and tables, it will be more difficult to parse than a well-formatted text file.  \n",
       "\n",
       "7. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** Some Nuances  \n",
       "   **Snippet:** While the steps for building a RAG system are conceptually simple, several nuances can make building one (in the real world) more complicated.  \n",
       "\n",
       "8. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** When NOT to Fine-tune  \n",
       "   **Snippet:** The effectiveness of any approach will depend on the details of the use case. For example, fine-tuning is less effective than retrieval augmented generation (RAG) to provide LLMs with specialized knowledge [1].  \n",
       "\n",
       "9. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** How it works  \n",
       "   **Snippet:** There are 2 key elements of a RAG system: a retriever and a knowledge base.  \n",
       "\n",
       "10. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** Why we care  \n",
       "   **Snippet:** Notice that RAG does not fundamentally change how we use an LLM; it's still prompt-in and response-out. RAG simply augments this process (hence the name).  \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(results_markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6442014-564f-4413-9c5f-d2e3b2e55d11",
   "metadata": {},
   "source": [
    "### 3) prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48874ba8-97a1-40ab-9364-90c2c7d96203",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = lambda query, results_markdown : f\"\"\" You are an AI assistant tasked with answering user questions based on excerpts from blog posts. Use the following snippets to \\\n",
    "provide accurate, concise, and synthesized answers. If the snippets don’t provide enough information, let the user know and suggest further exploration.\n",
    "\n",
    "## Question:\n",
    "{query}\n",
    "\n",
    "## Relevant Snippets:\n",
    "{results_markdown}\n",
    "\n",
    "---\n",
    "\n",
    "## Response:\n",
    "Provide a clear and concise response below, synthesizing information from the snippets and referencing them directly. If additional information is \\\n",
    "required, suggest further follow-ups or note what’s missing.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6022b678-3522-471f-b8b9-0f896e4eedb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template(query, results_markdown)\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0551c2a-fde3-4d48-a5d7-d7dfb6f362c4",
   "metadata": {},
   "source": [
    "### 4) prompt GPT-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab389a06-8359-49d1-96db-1663c2fdfc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make api call\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ], \n",
    "    temperature = 0.5\n",
    ")\n",
    "\n",
    "# extract response\n",
    "answer = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8da58-78a8-4136-8823-40e576d2115b",
   "metadata": {},
   "source": [
    "### 5) display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a4dbc7d-88c6-4a4d-9406-a96f16918f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "When does it make sense to use RAG vs fine-tuning?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "When deciding between using Retrieval-Augmented Generation (RAG) and fine-tuning, it's important to consider the specific use case and the desired outcome. RAG is particularly effective when you have a domain-specific knowledge base, such as internal company documents, and need to inject relevant context into an LLM's input prompt to generate more accurate and helpful responses (Snippet 4). This approach is often more effective than fine-tuning for providing LLMs with specialized knowledge (Snippets 3 and 8).\n",
       "\n",
       "Fine-tuning, on the other hand, adapts an existing model for a particular use case by training it further on a specific dataset. While this can be useful, it's generally considered less effective than RAG for endowing an LLM with specialized knowledge (Snippets 3 and 8).\n",
       "\n",
       "It's also worth noting that these approaches are not mutually exclusive and can be combined. For example, the original RAG system proposed by Facebook researchers utilized fine-tuning to better use retrieved information for generating responses (Snippet 5).\n",
       "\n",
       "For a more detailed understanding of when to use each approach, further exploration of the specific nuances and requirements of your use case would be beneficial, as the effectiveness of each method can depend heavily on those details (Snippets 1 and 8)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print()\n",
    "print(query)\n",
    "print()\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4291b35f-e2c5-4fca-b4e6-d674a99f0367",
   "metadata": {},
   "source": [
    "### Bonus: Streamline Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c238da9-3bf7-4333-965c-285be39218b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The benefits of fine-tuning large language models (LLMs) include:\n",
       "\n",
       "1. **Improved Performance for Specific Use Cases**: Fine-tuning allows smaller models to outperform larger pre-trained models on specific tasks, especially when tailored with high-quality datasets (Snippets 6, 10).\n",
       "\n",
       "2. **Lower Inference Costs**: Fine-tuning can reduce the computational costs associated with inference, making it a more efficient option for deploying AI assistants (Snippet 9).\n",
       "\n",
       "3. **Customization**: Fine-tuning enables the adaptation of LLMs to meet the unique requirements of particular applications, enhancing their relevance and effectiveness (Snippet 6).\n",
       "\n",
       "However, it's important to note that fine-tuning is not universally superior to other techniques like prompt engineering or retrieval augmented generation (RAG), and it may come with trade-offs, such as performance drops in certain tasks (Snippets 1, 5). Additionally, the quality of the training dataset is crucial for achieving optimal performance (Snippet 7).\n",
       "\n",
       "For a deeper understanding, you might explore specific use cases or the technical aspects of fine-tuning further."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What are the benefits of LLM fine-tuning?\"\n",
    "results_markdown = semantic_search(query, model, chunk_embeddings, chunk_list, temp=0.1, k=10, threshold=0.01)\n",
    "answer = answer_query(query, results_markdown, prompt_template, client)\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3a2383-fdc7-40ab-a5f6-f4e44cf6d287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
