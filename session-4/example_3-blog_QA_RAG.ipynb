{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22c6a7a2-26c9-4e14-aafa-5cf069d02e46",
   "metadata": {},
   "source": [
    "# Article Series QA Assistant with RAG\n",
    "## ABB #3 - Session 4\n",
    "\n",
    "Code authored by: Shaw Talebi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdff7bfa-ebdb-4323-b16a-d69aa948eb5b",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3ebb26-94ac-4c47-a5fc-170c8caa0cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "from functions import *\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "953a58cc-0b6b-4529-a2af-c1f58facd0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sk from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# setup api client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe890f-5c07-44c9-8afb-dbc819e82660",
   "metadata": {},
   "source": [
    "### load data & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029c926b-6f42-4af6-816a-e34dc20aab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load chunks\n",
    "filename = 'data/chunk_list.json'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    chunk_list = json.load(f)\n",
    "\n",
    "# load embeddings\n",
    "chunk_embeddings = torch.load('data/chunk_embeddings.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07c9b8c5-105a-492e-a14e-fb711e735f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num chunks: 778\n",
      "(778, 384)\n"
     ]
    }
   ],
   "source": [
    "print(\"Num chunks:\",len(chunk_list))\n",
    "print(chunk_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fff575e-faed-4d42-9c19-c4325f1e8e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109f511-8897-4e69-aa99-30a2ff79b0c9",
   "metadata": {},
   "source": [
    "### 1) define query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da59e64-ba7b-4150-8615-e23fe9b71a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define query\n",
    "query = \"When does it make sense to use RAG vs fine-tuning?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba55143-e771-4603-98a6-80b9f758d26d",
   "metadata": {},
   "source": [
    "### 2) context retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce377b19-c0ec-4519-9cd1-e119911f109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_markdown = semantic_search(query, model, chunk_embeddings, chunk_list, temp=0.1, k=10, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d16694e7-6dc2-4630-80cb-e1d087e38f7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** We’ve already mentioned situations where RAG and fine-tuning perform well. However, since this is such a common question, it’s worth reemphasizing when each approach works best.  \n",
       "\n",
       "2. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** Here’s high-level guidance on when to use each.  \n",
       "\n",
       "3. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** Why we care  \n",
       "   **Snippet:** Previous articles in this series discussed fine-tuning, which adapts an existing model for a particular use case. While this is an alternative way to endow an LLM with specialized knowledge, empirically, fine-tuning seems to be less effective than RAG at doing this [1].  \n",
       "\n",
       "4. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** RAG is when we inject relevant context into an LLM’s input prompt so that it can generate more helpful responses. For example, if we have a domain-specific knowledge base (e.g., internal company documents and emails), we might identify the items most relevant to the user’s query so that an LLM can synthesize information in an accurate and digestible way.  \n",
       "\n",
       "5. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** Notice that these approaches are not mutually exclusive. In fact, the original RAG system proposed by Facebook researchers used fine-tuning to better use retrieved information for generating responses [4].  \n",
       "\n",
       "6. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** Some Nuances  \n",
       "   **Snippet:** Document preparation—The quality of a RAG system is driven by how well useful information can be extracted from source documents. For example, if a document is unformatted and full of images and tables, it will be more difficult to parse than a well-formatted text file.  \n",
       "\n",
       "7. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** Some Nuances  \n",
       "   **Snippet:** While the steps for building a RAG system are conceptually simple, several nuances can make building one (in the real world) more complicated.  \n",
       "\n",
       "8. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** When NOT to Fine-tune  \n",
       "   **Snippet:** The effectiveness of any approach will depend on the details of the use case. For example, fine-tuning is less effective than retrieval augmented generation (RAG) to provide LLMs with specialized knowledge [1].  \n",
       "\n",
       "9. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** How it works  \n",
       "   **Snippet:** There are 2 key elements of a RAG system: a retriever and a knowledge base.  \n",
       "\n",
       "10. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** Why we care  \n",
       "   **Snippet:** Notice that RAG does not fundamentally change how we use an LLM; it's still prompt-in and response-out. RAG simply augments this process (hence the name).  \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(results_markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6442014-564f-4413-9c5f-d2e3b2e55d11",
   "metadata": {},
   "source": [
    "### 3) prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48874ba8-97a1-40ab-9364-90c2c7d96203",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = lambda query, results_markdown : f\"\"\"You are an AI assistant tasked with answering user questions based on excerpts from blog posts. Use the following snippets to \\\n",
    "provide accurate, concise, and synthesized answers. If the snippets don’t provide enough information, let the user know and suggest further exploration.\n",
    "\n",
    "## Question:\n",
    "{query}\n",
    "\n",
    "## Relevant Snippets:\n",
    "{results_markdown}\n",
    "\n",
    "---\n",
    "\n",
    "## Response:\n",
    "Provide a clear and concise response below, synthesizing information from the snippets and referencing them directly. If additional information is \\\n",
    "required, suggest further follow-ups or note what’s missing.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6022b678-3522-471f-b8b9-0f896e4eedb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = prompt_template(query, results_markdown)\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0551c2a-fde3-4d48-a5d7-d7dfb6f362c4",
   "metadata": {},
   "source": [
    "### 4) prompt GPT-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab389a06-8359-49d1-96db-1663c2fdfc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make api call\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": prompt},\n",
    "    ], \n",
    "    temperature = 0.5\n",
    ")\n",
    "\n",
    "# extract response\n",
    "answer = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8da58-78a8-4136-8823-40e576d2115b",
   "metadata": {},
   "source": [
    "### 5) display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a4dbc7d-88c6-4a4d-9406-a96f16918f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "When does it make sense to use RAG vs fine-tuning?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "When deciding between Retrieval Augmented Generation (RAG) and fine-tuning for enhancing language models, the choice largely depends on the specific use case.\n",
       "\n",
       "RAG is particularly effective when you have a domain-specific knowledge base, such as internal company documents or emails, and you need to inject relevant context into the model's input prompt to generate more accurate and helpful responses (Snippet 4). It works well for providing specialized knowledge to language models, as it empirically seems to be more effective than fine-tuning for this purpose (Snippet 3, 8).\n",
       "\n",
       "Fine-tuning, on the other hand, involves adapting an existing model for a particular use case by training it on specific data (Snippet 3). While it can endow a model with specialized knowledge, it may not be as effective as RAG in scenarios where the retrieval of context from a knowledge base is crucial.\n",
       "\n",
       "Moreover, these approaches are not mutually exclusive and can be combined. For instance, the original RAG system proposed by Facebook researchers used fine-tuning to better utilize retrieved information for generating responses (Snippet 5).\n",
       "\n",
       "In summary, use RAG when you need to leverage a large, domain-specific knowledge base to provide context to the model. Consider fine-tuning when you need to adapt a model to specific data, though it may be less effective for specialized knowledge compared to RAG. For complex scenarios, a combination of both approaches might be beneficial."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print()\n",
    "print(query)\n",
    "print()\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4291b35f-e2c5-4fca-b4e6-d674a99f0367",
   "metadata": {},
   "source": [
    "### Bonus: Streamline Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c238da9-3bf7-4333-965c-285be39218b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Fine-tuning large language models (LLMs) offers several key benefits:\n",
       "\n",
       "1. **Improved Performance for Specific Tasks**: Fine-tuned models can outperform larger pre-trained models when tailored for particular use cases, even with clever prompt engineering (Snippet 6). This customization allows the model to better understand and generate relevant outputs based on the specific context of the task.\n",
       "\n",
       "2. **Lower Inference Costs**: Fine-tuning can help reduce inference costs, making it more efficient compared to using a generic model (Snippet 9).\n",
       "\n",
       "3. **Adaptability to Quality Data**: The performance of a fine-tuned model is heavily influenced by the quality of the training dataset. A well-curated dataset can significantly enhance the model's effectiveness (Snippet 7).\n",
       "\n",
       "4. **Access to Open-Source Resources**: With the availability of numerous open-source resources, it has become easier for users to fine-tune models for their custom applications (Snippet 6).\n",
       "\n",
       "However, it's important to note that fine-tuning is not always the best approach for every scenario. It can come with an \"alignment tax,\" where performance may drop in some tasks compared to the base model (Snippet 5). Additionally, it may not be as effective as other techniques like retrieval-augmented generation (RAG) for providing specialized knowledge (Snippet 1).\n",
       "\n",
       "If you need more detailed insights or specific examples of use cases for fine-tuning, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What are the benefits of LLM fine-tuning?\"\n",
    "results_markdown = semantic_search(query, model, chunk_embeddings, chunk_list, temp=0.1, k=10, threshold=0.01)\n",
    "answer = answer_query(query, results_markdown, prompt_template, client)\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3a2383-fdc7-40ab-a5f6-f4e44cf6d287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
