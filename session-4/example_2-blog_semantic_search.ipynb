{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44353ae1-41a5-4e8e-bdfb-fec1f445f239",
   "metadata": {},
   "source": [
    "# Semantic Search with Text Embeddings\n",
    "## ABB #3 - Session 4\n",
    "\n",
    "Code authored by: Shaw Talebi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22ee10-b6c8-4e59-babd-366b41f4a357",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c3df73-038b-44b9-9bf2-526dc485c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from IPython.display import display, Markdown\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3667ac64-c7f7-4a29-9bf4-9eda692145c8",
   "metadata": {},
   "source": [
    "### 1) chunk articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53d05bc-7ace-455a-a6bd-bdf45116d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all HTML files from raw directory\n",
    "filename_list = [\"articles/\"+f for f in os.listdir('articles')]\n",
    "\n",
    "chunk_list = []\n",
    "for filename in filename_list:\n",
    "    # only process .html files\n",
    "    if filename.lower().endswith(('.html')):\n",
    "        # read html file\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "    \n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Get article title\n",
    "        article_title = soup.find('title').get_text().strip() if soup.find('title') else \"Untitled\"\n",
    "        \n",
    "        # Initialize variables\n",
    "        article_content = []\n",
    "        current_section = \"Main\"  # Default section if no headers found\n",
    "        \n",
    "        # Find all headers and text content\n",
    "        content_elements = soup.find_all(['h1', 'h2', 'h3', 'p', 'ul', 'ol'])\n",
    "    \n",
    "        # iterate through elements and extract text with metadata\n",
    "        for element in content_elements:\n",
    "            if element.name in ['h1', 'h2', 'h3']:\n",
    "                current_section = element.get_text().strip()\n",
    "            elif element.name in ['p', 'ul', 'ol']:\n",
    "                text = element.get_text().strip()\n",
    "                # Only add non-empty content that's at least 30 characters long\n",
    "                if text and len(text) >= 30:\n",
    "                    article_content.append({\n",
    "                        'article_title': article_title,\n",
    "                        'section': current_section,\n",
    "                        'text': text\n",
    "                    })\n",
    "    \n",
    "        # add article content to list\n",
    "        chunk_list.extend(article_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70b7fbf-c363-4a5a-84b8-be342a450dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save chunk list to file\n",
    "filename='data/chunk_list.json'\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunk_list, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e50b033-8519-4b93-b8ac-b05eed841604",
   "metadata": {},
   "source": [
    "### 2) embed chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52e1ddfa-8691-4cb0-8eef-74d7dc7db933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num chunks: 778\n"
     ]
    }
   ],
   "source": [
    "# define text to embed\n",
    "text_list = []\n",
    "for content in chunk_list:\n",
    "    # concatenate title and section header\n",
    "    context = content['article_title'] + \" - \" + content['section'] + \": \"\n",
    "    # append text from paragraph to fill CLIP's 256 sequence limit\n",
    "    text = context + content['text'][:512-len(context)]\n",
    "    \n",
    "    text_list.append(text)\n",
    "print(\"Num chunks:\",len(text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdd448c0-f1f2-476d-b6cf-415b19cf399a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article_title': 'Fine-Tuning BERT for Text Classification',\n",
       " 'section': 'Fine-Tuning BERT for Text Classification',\n",
       " 'text': 'Although today‚Äôs 100B+ parameter transformer models are state-of-the-art in AI, there‚Äôs still much we can accomplish with smaller (< 1B parameter) models. In this article, I will walk through one such example, fine-tuning BERT (110M parameters) to classify phishing URLs. I‚Äôll start by covering key concepts and then share example Python code.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450f026c-153d-4894-a03e-9eda2ee32455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fine-Tuning BERT for Text Classification - Fine-Tuning BERT for Text Classification: Although today‚Äôs 100B+ parameter transformer models are state-of-the-art in AI, there‚Äôs still much we can accomplish with smaller (< 1B parameter) models. In this article, I will walk through one such example, fine-tuning BERT (110M parameters) to classify phishing URLs. I‚Äôll start by covering key concepts and then share example Python code.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d03fe32a-230a-416a-ac7b-a25d85a41b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(778, 384)\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "# compute embeddings\n",
    "chunk_embeddings = model.encode(text_list)\n",
    "print(chunk_embeddings.shape)\n",
    "\n",
    "# save chunk embeddings to file\n",
    "torch.save(chunk_embeddings, 'data/chunk_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac9ac6-134e-421c-904e-ec6bf81834db",
   "metadata": {},
   "source": [
    "### 3) semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43e73516-fdfd-4963-ad19-2c3e413ab5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384,)\n",
      "torch.Size([1, 778])\n"
     ]
    }
   ],
   "source": [
    "# define query\n",
    "query = \"What is a token?\"\n",
    "query_embedding = model.encode(query)\n",
    "print(query_embedding.shape)\n",
    "\n",
    "# compute similarity between query and all chunks\n",
    "similarities = model.similarity(query_embedding, chunk_embeddings)\n",
    "print(similarities.shape)\n",
    "# print(similarities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3aaf616-c522-45c1-a89b-3af6ae4166bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search parameters\n",
    "temp = 0.1\n",
    "k=3\n",
    "threshold = 0.05\n",
    "\n",
    "# Rescale similarities via softmax\n",
    "scores = torch.nn.functional.softmax(similarities/temp, dim=1)\n",
    "\n",
    "# Get sorted indices and scores\n",
    "sorted_indices = scores.argsort(descending=True)[0]\n",
    "sorted_scores = scores[0][sorted_indices]\n",
    "\n",
    "# Filter by threshold and get top k\n",
    "filtered_indices = [\n",
    "    idx.item() for idx, score in zip(sorted_indices, sorted_scores) \n",
    "    if score.item() >= threshold\n",
    "][:k]\n",
    "\n",
    "# Get corresponding content items and scores\n",
    "top_results = [chunk_list[i] for i in filtered_indices]\n",
    "result_scores = [scores[0][i].item() for i in filtered_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "033e5528-44a0-4e7c-8cec-9eb4baeb77b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'article_title': 'Cracking Open the OpenAI (Python) API',\n",
       "  'section': '2) OpenAI‚Äôs (Python)\\xa0API',\n",
       "  'text': 'Tokens, in the context of LLMs, are essentially a set of numbers representing a set of words and characters. For example, ‚ÄúThe‚Äù could be a token, ‚Äú end‚Äù (with the space) could be another, and ‚Äú.‚Äù another.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ada7d8b5-73a5-4533-b79c-689dbb088acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.20563369989395142]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d785f-a1df-40aa-b762-2b79f3748df5",
   "metadata": {},
   "source": [
    "### 4) display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "246b8d0f-beea-406b-81b0-484171fb6231",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_markdown = \"\"\n",
    "for i, result in enumerate(top_results, start=1):\n",
    "    results_markdown += f\"{i}. **Article title:** {result['article_title']}  \\n\"\n",
    "    results_markdown += f\"   **Section:** {result['section']}  \\n\"\n",
    "    results_markdown += f\"   **Snippet:** {result['text']}  \\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97df4231-af86-4dd3-b806-cab95bb4b5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. **Article title:** Cracking Open the OpenAI (Python) API  \n",
       "   **Section:** 2) OpenAI‚Äôs (Python)¬†API  \n",
       "   **Snippet:** Tokens, in the context of LLMs, are essentially a set of numbers representing a set of words and characters. For example, ‚ÄúThe‚Äù could be a token, ‚Äú end‚Äù (with the space) could be another, and ‚Äú.‚Äù another.  \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(results_markdown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b101df6f-93a0-4a32-a362-e067397a9140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. **Article title:** How to Build an LLM from Scratch  \n",
       "   **Section:** Step 2: Model Architecture  \n",
       "   **Snippet:** Attention allows the neural network to capture the importance of content and position for modeling language. This has been an idea in ML for decades. However, the major innovation of the Transformer‚Äôs attention mechanism is computations can be done in parallel, providing significant speed-ups compared to recurrent neural networks, which rely on serial computations [13].  \n",
       "\n",
       "2. **Article title:** How to Build an LLM from Scratch  \n",
       "   **Section:** Step 2: Model Architecture  \n",
       "   **Snippet:** A transformer is a neural network architecture that uses attention mechanisms to generate mappings between inputs and outputs. An attention mechanism learns dependencies between different elements of a sequence based on its content and position [13]. This comes from the intuition that with language, context matters.  \n",
       "\n",
       "3. **Article title:** How to Build an LLM from Scratch  \n",
       "   **Section:** Step 2: Model Architecture  \n",
       "   **Snippet:** Encoder-Decoder‚Ää‚Äî‚Ääwe can combine the encoder and decoder modules to create an encoder-decoder transformer. This was the architecture proposed in the original ‚ÄúAttention is all you need‚Äù paper [13]. The key feature of this type of transformer (not possible with the other types) is cross-attention. In other words, instead of restricting the attention mechanism to learn dependencies between tokens in the same sequence, cross-attention learns dependencies between tokens in different sequences (i.e. sequences from encoder and decoder modules). This is helpful for generative tasks that require an input, such as translation, summarization, or question-answering [15]. Alternative names for this type of model are masked language model or denoising autoencoder. A popular LLM using this design is Facebook‚Äôs BART [17].  \n",
       "\n",
       "4. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** Resources  \n",
       "   **Snippet:** Socials: YouTube üé• | LinkedIn | Instagram  \n",
       "\n",
       "5. **Article title:** Cracking Open the Hugging Face Transformers Library  \n",
       "   **Section:** What is Hugging¬†Face?  \n",
       "   **Snippet:** The power of these resources is that they are community generated, which leverages all the benefits of open-source (i.e. cost-free, wide diversity of tools, high-quality resources, and rapid pace of innovation). While these make building powerful ML projects more accessible than before, there is another key element of the Hugging Face ecosystem‚Ää‚Äî‚Ääthe Transformers library.  \n",
       "\n",
       "6. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** Why we¬†care  \n",
       "   **Snippet:** Notice that RAG does not fundamentally change how we use an LLM; it's still prompt-in and response-out. RAG simply augments this process (hence the name).  \n",
       "\n",
       "7. **Article title:** Cracking Open the Hugging Face Transformers Library  \n",
       "   **Section:** Conclusion  \n",
       "   **Snippet:** Hugging Face has become synonymous with open-source language models and machine learning. The biggest advantage of their ecosystem is it gives small-time developers, researchers, and tinkers access to powerful ML resources.  \n",
       "\n",
       "8. **Article title:** A Practical Introduction to LLMs  \n",
       "   **Section:** Resources  \n",
       "   **Snippet:** Socials: YouTube üé• | LinkedIn | Twitter  \n",
       "\n",
       "9. **Article title:** Multimodal Models‚Ää‚Äî‚ÄäLLMs that can see and hear  \n",
       "   **Section:** Example: Using LLaMA 3.2 Vision for Image-based Tasks  \n",
       "   **Snippet:** Objectively describing a scene is simpler than understanding and explaining humor. Let‚Äôs see how the model explains the meme below.  \n",
       "\n",
       "10. **Article title:** Cracking Open the OpenAI (Python) API  \n",
       "   **Section:** Resources  \n",
       "   **Snippet:** Socials: YouTube üé• | LinkedIn | Twitter  \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bringing it all together\n",
    "query = \"What is attention?\"\n",
    "results_markdown = semantic_search(query, model, chunk_embeddings, chunk_list, temp=0.1, k=10, threshold=0)\n",
    "display(Markdown(results_markdown))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
